import json
import math
import random
import threading
import uuid
import logging
from multiprocessing import Pool

import requests
import ujson
import time as tm

from kafka import KafkaProducer
from kafka.errors import KafkaTimeoutError, KafkaError
from pip._internal.cli.cmdoptions import retries
from tqdm import tqdm
from minio import Minio, S3Error
from datetime import timedelta, datetime, time

import pandas as pd
import psycopg2
import pymssql
from psycopg2.extras import RealDictCursor, execute_values
from typing import Optional, List, Dict, Any
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)
pd.set_option('display.max_colwidth', 200)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('spider_amap_weather.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# å‚æ•°
log_web_page = ['login', 'home', 'product_list', 'product_detail', 'search', 'payment']
search_list = ['è¿åŠ¨å¥—è£…', 'ç‘œä¼½æœ', 'èƒŒå¿ƒ', 'æ–œæŒåŒ…', 'ä¼‘é—²è¡«', 'è¿åŠ¨å¤´å¸¦', 'å‘åœˆ', 'ç‘œä¼½è£¤', 'è¿åŠ¨å†…è¡£', 'å†¬è£…', 'ç¾½ç»’']
user_device_change_rate = 0.1

# postgres conf
postgresql_ip = 'cdh01'
postgresql_port = 5432
postgresql_user_name = 'postgres'
postgresql_user_pwd = '123456'
postgresql_db = 'spider_db'
postgresql_db_schema = 'public'




# sqlserver conf
sqlserver_ip = '192.168.200.102'
sqlserver_port = "1433"
sqlserver_user_name = 'sa'
sqlserver_user_pwd = 'Xy0511./'
sqlserver_db = 'realtime_v3'
sqlserver_db_schema = 'dbo'
sqlserver_db_jdbc_url = 'jdbc:sqlserver://192.168.200.102:1433'

# minio conf
minio_endpoint = ""
minio_secure = False
minio_access_key = ''
minio_secret_key = ''
bucket_name = ""
folder_prefix = ""

# mysql conf
mysql_ip = 'cdh03'
mysql_port = "3306"
mysql_user_name = 'root'
mysql_user_pwd = 'root'
mysql_db = 'realtime_v3'

# kafka
kafka_log_topic = 'realtime_v3_logs'
kafka_bootstrap_servers = 'cdh01:9092,cdh02:9092,cdh03:9092'
kafka_batch_size = 100

minio_client = Minio(
    endpoint=minio_endpoint,
    access_key=minio_access_key,
    secret_key=minio_secret_key,
    secure=minio_secure
)


def process_thread_func(v_func_dict_list, v_type, v_process_cnt=1, v_thread_max=21):
    """
        æ‰¹é‡çº¿ç¨‹æ‰§è¡Œæ–¹æ³•
        :param v_func_dict_list:
        :param v_thread_max:
        :param v_func_list_dict ä¼ å…¥æ–¹æ³•listã€å‚æ•°é›†åˆ exp:[{'func_name':'arg1_name,arg2_name'},{'func_name1':'arg1_name,arg2_name'}]
        :param v_type çº¿ç¨‹ thread è¿›ç¨‹ process
        :param v_process_cnt è¿›ç¨‹æ•° v_thread_max çº¿ç¨‹æ•°
    """
    thread_list = []
    # è®¾ç½®æœ€å¤§çº¿ç¨‹æ•°
    thread_max = v_thread_max
    if v_func_dict_list and v_type == 'thread':
        for i in v_func_dict_list:
            func_name = i['func_name']
            func_args = i['func_args']
            t = threading.Thread(target=func_name, name=i, args=(func_args))
            t.start()
            thread_list.append(t)
            logger.info('çº¿ç¨‹æ–¹æ³• {} å·²è¿è¡Œ'.format(func_name.__name__))
            if threading.active_count() == thread_max:
                for j in thread_list:
                    j.join()
                thread_list = []
        for j in thread_list:
            j.join()
        logger.info('æ‰€æœ‰çº¿ç¨‹æ–¹æ³•æ‰§è¡Œç»“æŸ')

    elif v_func_dict_list and v_type == 'process':
        pool = Pool(processes=v_process_cnt)
        for i in v_func_dict_list:
            try:
                func_name = i['func_name']
                func_args = i['func_args']
                pool.apply_async(func=func_name, args=func_args)
                logger.info('è¿›ç¨‹æ–¹æ³• {} å·²è¿è¡Œ'.format(func_name.__name__))
            except Exception as e:
                logger.info('è¿›ç¨‹æŠ¥é”™:', e)
        pool.close()
        pool.join()
        logger.info('æ‰€æœ‰è¿›ç¨‹æ–¹æ³•æ‰§è¡Œç»“æŸ')


def write_orders_to_sqlserver(order_list):
    """æ‰¹é‡å†™å…¥è®¢å•æ•°æ®åˆ°SQL Serveræ•°æ®åº“"""
    conn = None
    try:
        import pyodbc

        # ğŸ”¥ ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨ ODBC Driver 18ï¼ˆæ‚¨å®‰è£…çš„ç‰ˆæœ¬ï¼‰
        conn_str = (
            f"DRIVER={{ODBC Driver 18 for SQL Server}};"  # æ”¹ä¸º 18
            f"SERVER={sqlserver_ip},{sqlserver_port};"
            f"DATABASE={sqlserver_db};"
            f"UID={sqlserver_user_name};"
            f"PWD={sqlserver_user_pwd};"
            "TrustServerCertificate=yes;"  # é‡è¦ï¼šå¤„ç†è¯ä¹¦é—®é¢˜
            "Encrypt=yes;"  # ODBC 18 é»˜è®¤è¦æ±‚åŠ å¯†
        )
        conn = pyodbc.connect(conn_str)
        cursor = conn.cursor()

        # åˆ›å»ºæ’å…¥SQLï¼ˆä½¿ç”¨?ä½œä¸ºå‚æ•°å ä½ç¬¦ï¼‰
        insert_sql = """
        INSERT INTO oms_order_dtl (
            order_id, user_id, user_name, phone_number, product_link, 
            product_id, color, size, item_id, material, sale_num, 
            sale_amount, total_amount, product_name, is_online_sales, 
            shipping_address, recommendations_product_ids, ds, ts
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """

        # å‡†å¤‡æ‰¹é‡æ•°æ®
        data = []
        for order in order_list:
            # å¤„ç†recommendations_product_idså­—æ®µ
            rec_ids = order["recommendations_product_ids"]
            if isinstance(rec_ids, list):
                rec_ids = ','.join(rec_ids)
            elif rec_ids is None:
                rec_ids = ''

            # å¤„ç†å¸ƒå°”å€¼å­—æ®µ
            is_online = 1 if order["is_online_sales"] else 0

            row = (
                str(order["order_id"]),
                str(order["user_id"]),
                str(order["user_name"]),
                str(order["phone_number"]),
                str(order["product_link"])[:255],
                str(order["product_id"]),
                str(order["color"]),
                str(order["size"]),
                str(order["item_id"]) if order["item_id"] else '',
                str(order["material"]) if order["material"] else '',
                int(order["sale_num"]),
                float(order["sale_amount"]),
                float(order["total_amount"]),
                str(order["product_name"])[:255],
                is_online,
                str(order["shipping_address"])[:255],
                str(rec_ids)[:1000],
                order["ds"],
                int(float(order["ts"]))
            )
            data.append(row)

        # æ‰§è¡Œæ‰¹é‡æ’å…¥
        cursor.executemany(insert_sql, data)
        conn.commit()

        print(f"âœ… æˆåŠŸæ’å…¥ {len(order_list)} æ¡è®¢å•è®°å½•åˆ° SQL Server")

    except Exception as ee:
        print(f"âŒ å†™å…¥SQL Serveræ•°æ®åº“æ—¶å‘ç”Ÿé”™è¯¯: {ee}")
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()

def generate_random_list(original_list, min_length=1, max_length=None):
    if max_length is None:
        max_length = len(original_list)
    length = random.randint(min_length, max(max_length, min_length))
    return random.sample(original_list, k=length)


def get_postgres_data(
        host: str = postgresql_ip,
        port: int = postgresql_port,
        dbname: str = postgresql_db,
        user: str = postgresql_user_name,
        password: str = postgresql_user_pwd,
        query: Optional[str] = None,
        return_df: bool = True
) -> pd.DataFrame | List[Dict[str, Any]]:
    conn = psycopg2.connect(
        host=host,
        port=port,
        dbname=dbname,
        user=user,
        password=password
    )

    try:
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute(query)
            results = cursor.fetchall()

            if return_df:
                return pd.DataFrame(results)
            else:
                return results

    except Exception as ae:
        print(f"æŸ¥è¯¢æ•°æ®åº“æ—¶å‘ç”Ÿé”™è¯¯: {ae}")
        raise
    finally:
        if conn is not None:
            conn.close()


def generate_uuid_order_id():
    return uuid.uuid4().hex


def collect_minio_user_device_2postgresql():
    all_records = []  # å­˜å‚¨æ‰€æœ‰è§£æåçš„è®°å½•
    total_bytes_processed = 0  # å·²å¤„ç†çš„å­—èŠ‚æ•°
    total_file_size = 0  # æ‰€æœ‰æ–‡ä»¶çš„æ€»å¤§å°

    try:
        if not minio_client.bucket_exists(bucket_name=bucket_name):
            logger.error(f"bucket {bucket_name} don't exists ! ")
            return

        # è®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„æ€»å¤§å°ï¼Œç”¨äºè¿›åº¦è®¡ç®—
        objects = list(minio_client.list_objects(bucket_name, recursive=True))
        for obj in objects:
            total_file_size += obj.size

        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        with tqdm(total=total_file_size, desc="è¯»å–æ•°æ®", unit="B", unit_scale=True) as pbar:
            for obj in objects:
                file_name = obj.object_name
                file_size = obj.size
                logger.info(f"scan file: {file_name}, size: {file_size / (1024 * 1024):.2f} MB")

                response = minio_client.get_object(bucket_name, file_name)
                temp_buffer = ""

                # é€å—è¯»å–æ–‡ä»¶å†…å®¹
                for chunk in response.stream(8192):
                    chunk_size = len(chunk)
                    total_bytes_processed += chunk_size
                    pbar.update(chunk_size)

                    temp_buffer += chunk.decode('utf-8', errors='ignore')

                    # æŒ‰è¡Œå¤„ç†æ•°æ®
                    while '\n' in temp_buffer:
                        line, temp_buffer = temp_buffer.split('\n', 1)
                        try:
                            res_json = ujson.loads(line.strip())
                            res = {
                                "brand": res_json['brand'],
                                "plat": res_json['plat'],
                                "platv": res_json['platv'],
                                "softv": res_json['softv'],
                                "uname": res_json['uname'],
                                "userkey": res_json['userkey'],
                                "datatype": res_json['datatype'],
                                "device": res_json['device'],
                                "ip": res_json['ip'],
                                "net": res_json['net'],
                                "opa": res_json['opa']
                            }
                            all_records.append(res)
                        except Exception as e:
                            logger.warning(f"è§£æè¡Œæ•°æ®å¤±è´¥: {e}, è¡Œå†…å®¹: {line[:100]}...")

                response.close()
                response.release_conn()

                logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {file_name}")

        total_records = len(all_records)
        logger.info(f"å…±è§£æ {total_records} æ¡è®°å½•")

        if total_records == 0:
            logger.info("æ²¡æœ‰æ•°æ®éœ€è¦å†™å…¥æ•°æ®åº“")
            return

        # batch insert pg
        batch_size = 1000  # æ¯æ‰¹æ’å…¥çš„è®°å½•æ•°
        total_batches = (total_records + batch_size - 1) // batch_size
        inserted_count = 0

        start_time = tm.time()

        with psycopg2.connect(
                host=postgresql_ip,
                port=postgresql_port,
                dbname=postgresql_db,
                user=postgresql_user_name,
                password=postgresql_user_pwd
        ) as conn:
            with conn.cursor() as cursor:

                # ä½¿ç”¨tqdmæ˜¾ç¤ºæ’å…¥è¿›åº¦
                with tqdm(total=total_batches, desc="å†™å…¥æ•°æ®åº“", unit="æ‰¹") as pbar:
                    for i in range(0, total_records, batch_size):
                        batch = all_records[i:i + batch_size]

                        # å‡†å¤‡æ‰¹é‡æ’å…¥çš„æ•°æ®
                        values = [(
                            r['brand'],
                            r['plat'],
                            r['platv'],
                            r['softv'],
                            r['uname'],
                            r['userkey'],
                            r['datatype'],
                            r['device'],
                            r['ip'],
                            r['net'],
                            r['opa']
                        ) for r in batch]

                        # æ‰¹é‡æ’å…¥
                        # noinspection SqlResolve
                        insert_sql = """
                        INSERT INTO spider_db.public.user_device_base (
                            brand, plat, platv, softv, uname, userkey, datatype, device, ip, net, opa
                        ) VALUES %s
                        """

                        # ä½¿ç”¨psycopg2çš„execute_valuesè¿›è¡Œæ‰¹é‡æ’å…¥
                        from psycopg2.extras import execute_values
                        execute_values(cursor, insert_sql, values)

                        inserted_count += len(batch)

                        # æ›´æ–°è¿›åº¦æ¡
                        pbar.update(1)

                        # æ¯1%æ›´æ–°ä¸€æ¬¡è¿›åº¦æ¡æ˜¾ç¤º
                        progress_percentage = (inserted_count / total_records) * 100
                        if progress_percentage % 1 <= (len(batch) / total_records) * 100:
                            pbar.set_postfix({"è¿›åº¦": f"{progress_percentage:.1f}%"})

                conn.commit()

        end_time = tm.time()
        duration = end_time - start_time

        # è¾“å‡ºæ±‡æ€»ç»“æœ
        logger.info("===== æ±‡æ€»ç»“æœ =====")
        logger.info(f"æ€»è€—æ—¶: {duration:.2f} ç§’")
        logger.info(f"æ€»å¤„ç†æ–‡ä»¶æ•°: {len(objects)}")
        logger.info(f"æ€»æ’å…¥è®°å½•æ•°: {inserted_count}")
        logger.info(f"å¹³å‡æ’å…¥é€Ÿåº¦: {inserted_count / duration:.2f} æ¡/ç§’")
        logger.info("===================")

        push_msg = {
            "platform": 'Data Spy',
            "context": 'collect_minio_user_device_2postgresql',
            "total_time": str(f'{duration:.2f}'),
            "success_count": inserted_count,
            "fail_count": 0,
        }

        push_feishu_msg(push_msg)
        logger.info("push Message To Lark Suc ...")

    except S3Error as e:
        logger.error(f"MinIOé”™è¯¯: {e}")
    except Exception as e:
        logger.error(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")


def push_feishu_msg(msg) -> None:
    """
    æ¨é€æ¶ˆæ¯åˆ°é£ä¹¦
    """

    feishu_url = ''
    try:
        # å‘é€POSTè¯·æ±‚
        response = requests.post(
            url=feishu_url,
            headers={"Content-Type": "application/json"},
            data=json.dumps(msg),
            timeout=10
        )

        print(f"Response status code: {response.status_code}")
        if response.text:
            print(f"Response content: {response.text}")

    except Exception as e:
        print(f"Error sending message: {str(e)}")
        import traceback
        traceback.print_exc()


def generate_random_in_range(low, high, data_type='float', precision=None):
    """
    ç”ŸæˆæŒ‡å®šåŒºé—´çš„éšæœºæ•°

    å‚æ•°:
    low (int/float): åŒºé—´ä¸‹é™
    high (int/float): åŒºé—´ä¸Šé™
    data_type (str): æ•°æ®ç±»å‹ï¼Œ'int' æˆ– 'float' (é»˜è®¤)
    precision (int): æµ®ç‚¹æ•°ç²¾åº¦ï¼ˆå°æ•°ä½æ•°ï¼‰ï¼Œä»…å½“ data_type='float' æ—¶æœ‰æ•ˆ

    è¿”å›:
    int/float: æŒ‡å®šåŒºé—´å†…çš„éšæœºæ•°
    """
    # éªŒè¯è¾“å…¥
    # è‡ªåŠ¨äº¤æ¢ä¸Šä¸‹é™
    if low > high:
        low, high = high, low

    if not isinstance(low, (int, float)) or not isinstance(high, (int, float)):
        raise ValueError("low å’Œ high å¿…é¡»æ˜¯æ•°å­—ç±»å‹ (int æˆ– float)")

    # æ ¹æ®æ•°æ®ç±»å‹ç”Ÿæˆéšæœºæ•°
    if data_type == 'int':
        # æ•´æ•°ç±»å‹
        return random.randint(math.ceil(low), math.floor(high))
    elif data_type == 'float':
        result = random.uniform(low, high)
        # å¤„ç†ç²¾åº¦
        if precision is not None and precision >= 0:
            result = round(result, precision)

        return result
    else:
        raise ValueError("data_type å¿…é¡»æ˜¯ 'int' æˆ– 'float'")


def generate_ordered_timestamps(base_date_str: str, n: int) -> List[datetime]:
    """
    ç”Ÿæˆæ—¶é—´æˆ³åºåˆ—ï¼Œæ»¡è¶³ï¼š
    1. å¦‚æœæ˜¯ä»Šå¤©çš„æ—¥æœŸï¼Œæ—¶é—´æˆ³ä¸è¶…è¿‡å½“å‰æ—¶é—´
    2. å¦‚æœä¸æ˜¯ä»Šå¤©çš„æ—¥æœŸï¼Œæ—¶é—´æˆ³åœ¨ä¼ å…¥æ—¥æœŸçš„å½“å¤©
    3. æ—¶é—´æˆ³æ•´ä½“é€’å¢ï¼ˆå…è®¸10%çš„æ¦‚ç‡å‡ºç°æ¯”å‰ä¸€æ¡å°çš„æ—¶é—´æˆ³ï¼‰
    4. æœ‰5%çš„æ¦‚ç‡ä½¿ç”¨éå½“æ—¥æ—¶é—´æˆ³ï¼ˆå‰3å¤©å†…éšæœºæ—¥æœŸï¼‰
    """
    # è§£æåŸºå‡†æ—¥æœŸ
    base_date = datetime.strptime(base_date_str, "%Y%m%d").date()
    current_date = datetime.now().date()

    # è®¡ç®—æ—¶é—´èŒƒå›´
    if base_date == current_date:
        # ä»Šå¤©ï¼šæ—¶é—´æˆ³ä¸è¶…è¿‡å½“å‰æ—¶é—´
        start_of_day = datetime.combine(base_date, time.min)
        end_of_day = datetime.now()
    else:
        # éä»Šå¤©ï¼šä½¿ç”¨å…¨å¤©æ—¶é—´èŒƒå›´
        start_of_day = datetime.combine(base_date, time.min)
        end_of_day = datetime.combine(base_date, time.max)

    # å‰3å¤©çš„æ—¶é—´èŒƒå›´
    three_days_earliest = base_date - timedelta(days=3)
    start_of_three_days = datetime.combine(three_days_earliest, time.min)

    # ç”ŸæˆåŸºç¡€é€’å¢åºåˆ—
    total_seconds = (end_of_day - start_of_day).total_seconds()
    if n == 0:
        return []
    elif n == 1:
        random_points = [0.0]
    else:
        random_points = [0.0] + sorted([random.random() for _ in range(n - 1)])

    ts_ms = [start_of_day + timedelta(seconds=x * total_seconds) for x in random_points]

    # åˆ›å»ºä¹±åºå’Œéå½“æ—¥æ ‡è®°
    non_today_flags = [False] * n
    disorder_indices = []

    # é€‰æ‹©10%çš„ä¹±åºä½ç½®ï¼ˆè·³è¿‡ç¬¬ä¸€æ¡ï¼‰
    if n > 1:
        disorder_count = min(n - 1, max(1, int(0.1 * n)))
        disorder_indices = random.sample(range(1, n), disorder_count)
        # æŒ‰ç´¢å¼•é¡ºåºå¤„ç†
        disorder_indices.sort()

    # å¤„ç†ä¹±åºä½ç½®
    for idx in disorder_indices:
        prev_timestamp = ts_ms[idx - 1]
        prev_is_non_today = non_today_flags[idx - 1]

        # 5%æ¦‚ç‡ä½¿ç”¨éå½“æ—¥æ—¶é—´æˆ³
        use_non_today = random.random() < 0.05

        if use_non_today:
            # éå½“æ—¥æ—¶é—´æˆ³ï¼ˆå‰3å¤©å†…ï¼‰
            non_today_date = base_date - timedelta(days=random.randint(1, 3))
            non_today_start = datetime.combine(non_today_date, time.min)
            non_today_end = datetime.combine(non_today_date, time.max)

            # ç¡®ä¿ä¸è¶…è¿‡å‰3å¤©çš„èŒƒå›´
            new_timestamp = datetime.fromtimestamp(
                random.uniform(
                    max(start_of_three_days, non_today_start).timestamp(),
                    min(non_today_end, datetime.combine(base_date, time.min)).timestamp()
                )
            )
            non_today_flags[idx] = True
        else:
            # å½“æ—¥æ—¶é—´æˆ³ï¼ˆç¡®ä¿æ¯”å‰ä¸€æ¡å°ï¼‰
            max_timestamp = prev_timestamp if prev_is_non_today else prev_timestamp
            min_timestamp = start_of_day if not prev_is_non_today else start_of_three_days

            # ç”Ÿæˆæ¯”å‰ä¸€æ¡å°çš„æ—¶é—´æˆ³
            new_timestamp = datetime.fromtimestamp(
                random.uniform(
                    min_timestamp.timestamp(),
                    max_timestamp.timestamp()
                )
            )

        ts_ms[idx] = new_timestamp

    return ts_ms


def sample_random_items(lst: list, min_count: int = 100, max_count: int = 229) -> list:
    if min_count > max_count:
        min_count, max_count = max_count, min_count

    actual_max = min(max_count, len(lst))

    if len(lst) < min_count:
        return lst

    sample_size = random.randint(min_count, actual_max)
    return random.sample(lst, sample_size)


def sink_dict_2postgresql(sink_data=None, table_schema='public', table_name='', sink_fields=None):
    conn = None
    if not sink_data:
        raise ValueError("sink_data ä¸èƒ½ä¸ºç©º")
    if not table_name:
        raise ValueError("table_name ä¸èƒ½ä¸ºç©º")

    pg_origin_query_sql = f"""
        select
            column_name,
            data_type,
            character_maximum_length,
            is_nullable,
            column_default
        from information_schema.columns
        where table_schema = '{table_schema}' and table_name = '{table_name}';
    """
    origin_message_data = get_postgres_data(query=pg_origin_query_sql, return_df=False)
    table_columns = [i['column_name'] for i in origin_message_data]
    if not sink_fields:
        sink_fields = list(sink_data[0].keys())
    invalid_fields = [f for f in sink_fields if f not in table_columns]
    if invalid_fields:
        raise ValueError(f"å­—æ®µ {invalid_fields} ä¸å­˜åœ¨äºè¡¨ {table_schema}.{table_name} ä¸­")
    insert_values = []
    for item in sink_data:
        row = []
        for field in sink_fields:
            # å¤„ç†JSONç±»å‹å­—æ®µ
            if field == 'log' and isinstance(item.get(field), dict):
                row.append(json.dumps(item[field]))
            else:
                row.append(item.get(field))
        insert_values.append(tuple(row))
    columns_str = ', '.join(sink_fields)
    placeholders = ', '.join(['%s'] * len(sink_fields))
    insert_sql = f"""
        INSERT INTO {table_schema}.{table_name} ({columns_str})
        VALUES %s
    """
    try:
        conn = psycopg2.connect(
            host=postgresql_ip,
            port=postgresql_port,
            dbname=postgresql_db,
            user=postgresql_user_name,
            password=postgresql_user_pwd
        )

        with conn.cursor() as cursor:
            # ä½¿ç”¨execute_valuesè¿›è¡Œæ‰¹é‡æ’å…¥
            execute_values(cursor, insert_sql, insert_values)
            conn.commit()

        logger.info(f"æˆåŠŸæ’å…¥ {len(sink_data)} æ¡è®°å½•åˆ° {table_schema}.{table_name}")
    except Exception as e:
        logger.info(f"æ’å…¥å¤±è´¥: {e}")
        raise
    finally:
        if conn:
            conn.close()


def sink_dict_data_2kafka(sink_topic='', sink_data=None):

    if not sink_topic:
        raise ValueError("å¿…é¡»æŒ‡å®šç›®æ ‡Kafkaä¸»é¢˜ï¼ˆsink_topicï¼‰")
    if not sink_data or len(sink_data) == 0:
        logger.info("å¾…å‘é€æ•°æ®ä¸ºç©ºï¼Œæ— éœ€æ‰§è¡Œå‘é€")
        return

    try:
        producer = KafkaProducer(
            bootstrap_servers=kafka_bootstrap_servers,
            value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8'),
            retries=retries,  # å¤±è´¥é‡è¯•
            batch_size=kafka_batch_size,  # æ‰¹é‡å¤§å°
            linger_ms=3,  # å»¶è¿Ÿå‘é€ï¼ˆèšåˆæ‰¹é‡ï¼‰
            acks='all',
            max_block_ms=60000  # å‘é€é˜»å¡è¶…æ—¶æ—¶é—´
        )
    except Exception as e:
        logger.error(f"Kafkaç”Ÿäº§è€…åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise

    try:
        success_count = 0
        fail_count = 0
        # é€æ¡å‘é€ï¼ˆæˆ–æ‰¹é‡å¼‚æ­¥å‘é€ï¼‰
        for idx, data in enumerate(sink_data):
            try:
                # å¼‚æ­¥å‘é€å¹¶è·å–Futureå¯¹è±¡
                future = producer.send(
                    topic=sink_topic,
                    value=data  # è‡ªåŠ¨åºåˆ—åŒ–ä¸ºJSON
                )
                # åŒæ­¥ç­‰å¾…å‘é€ç»“æœï¼ˆå¦‚éœ€å¼‚æ­¥å¯å»æ‰get()ï¼‰
                record_metadata = future.get(timeout=10)
                success_count += 1
                # logger.info(
                #     f"æ•°æ®å‘é€æˆåŠŸ - ä¸»é¢˜: {record_metadata.topic}, "
                #     f"åˆ†åŒº: {record_metadata.partition}, "
                #     f"åç§»é‡: {record_metadata.offset}"
                # )
            except KafkaTimeoutError:
                fail_count += 1
                logger.warning(f"æ•°æ®å‘é€è¶…æ—¶ï¼ˆç´¢å¼•: {idx}ï¼‰: {data}")
            except KafkaError as e:
                fail_count += 1
                logger.error(f"Kafkaå‘é€å¤±è´¥ï¼ˆç´¢å¼•: {idx}ï¼‰: {str(e)}, æ•°æ®: {data}")
            except Exception as e:
                fail_count += 1
                logger.error(f"æœªçŸ¥é”™è¯¯ï¼ˆç´¢å¼•: {idx}ï¼‰: {str(e)}, æ•°æ®: {data}")

        # ç¡®ä¿æ‰€æœ‰æ•°æ®è¢«å‘é€
        producer.flush()
        logger.info(
            f"Kafkaå‘é€å®Œæˆ - æ€»æ¡æ•°: {len(sink_data)}, "
            f"æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count}"
        )

    finally:
        # å…³é—­ç”Ÿäº§è€…
        producer.close()
    pass


def business_mock_data(ds, data_model='batch'):
    # åªä¼ å…¥ä¸€ä¸ªæ—¥æœŸå‚æ•°ï¼Œæ ¼å¼å¦‚20250708
    # params_ds = "20250708"

    order_list = []
    all_logs_list = []

    # noinspection SqlResolve
    query_sql = """
        select  id, 
                product_link, 
                sale_amount, 
                product_desc, 
                product_id, 
                item_id, 
                material, 
                is_online_sales, 
                color_list, 
                size_list, 
                comments, 
                recommendations_product_id, 
                ts 
        from spider_db.public.spider_lululemon_jd_product_dtl 
        where product_id is not null 
        order by random() 
        limit (floor(random() * (110-80)) + 80);
    """

    # noinspection SqlResolve
    query_pg_device = """
        select brand, plat, platv, softv, uname, userkey, device, ip, net, opa
        from spider_db.public.user_device_base
        order by random()
        limit (floor(random() * (110-80)) + 500);
    """

    # nosql
    # noinspection SqlResolve
    user_query_sql = """
        select user_id,
               uname,
               phone_num,
               address
        from spider_db.public.user_info_base;
    """

    user_info_data = get_postgres_data(query=user_query_sql, return_df=False)
    selected_users = random.choices(user_info_data, k=min(30, len(user_info_data))) if user_info_data else []

    pg_device_data = get_postgres_data(query=query_pg_device, return_df=False)

    lst = get_postgres_data(query=query_sql, return_df=False)
    lens = len(lst)

    # ç”Ÿæˆæ—¶é—´æˆ³åºåˆ—
    timestamps = generate_ordered_timestamps(str(ds), lens)
    order_list = []

    if data_model == 'batch':
        for i, item in enumerate(lst):

            user = random.choice(selected_users) if selected_users else {
                "user_id": "default_user",
                "uname": "åŒ¿åç”¨æˆ·",
                "phone_num": "00000000000",
                "address": "æœªçŸ¥åœ°å€"
            }

            color_list = item['color_list']
            # color
            if color_list is not None and len(color_list) > 0:
                color = random.choices(color_list)[0]
            else:
                color = '/'
            size_list = item['size_list']
            if size_list is not None and len(size_list) > 0:
                size = random.choices(size_list)[0]
            else:
                size = 'å‡ç '

            product_link = item['product_link']
            product_id = item['product_id']
            item_id = item['item_id']
            material = item['material']
            sale_amount = item.get("sale_amount")
            product_name = item['product_desc']
            is_online_sales = item['is_online_sales']
            recommendations_product_ids = item['recommendations_product_id']

            # è·å–æ—¶é—´æˆ³
            timestamp = timestamps[i]
            ts = int(timestamp.timestamp())  # ç§’çº§æ—¶é—´æˆ³
            ds = timestamp.strftime('%Y-%m-%d %H:%M:%S')  # æ—¥æœŸå­—ç¬¦ä¸²
            sale_num = generate_random_in_range(1, 9, data_type="int")

            order_info = {
                "order_id": generate_uuid_order_id(),
                "user_id": user["user_id"],
                "user_name": user["uname"],
                "phone_number": user["phone_num"],
                "product_link": product_link,
                "product_id": product_id,
                "color": color,
                "size": size,
                "item_id": item_id,
                "material": material,
                "sale_num": sale_num,
                "sale_amount": sale_amount,
                "total_amount": sale_num * sale_amount,
                "product_name": product_name,
                "is_online_sales": is_online_sales,
                "shipping_address": user["address"],
                "recommendations_product_ids": recommendations_product_ids,
                "ds": ds,
                "ts": ts,
            }
            order_list.append(order_info)
            device_info_main = random.choice(pg_device_data) if pg_device_data else {
                'brand': 'Unknown',
                'plat': 'Unknown',
                'platv': 'Unknown',
                'softv': 'Unknown',
                'uname': 'Unknown',
                'userkey': 'Unknown',
                'device': 'Unknown',
                'ip': '0.0.0.0',
                'net': 'Unknown',
                'opa': 'Unknown'
            }
            use_variation = random.random() < user_device_change_rate
            log_count = random.randint(2, 6)
            required_logs = ['payment']
            other_logs = random.choices(
                [log for log in log_web_page if log != 'payment'],
                k=log_count - 1
            )
            log_types = required_logs + other_logs
            random.shuffle(log_types)
            log_timestamps = sorted(
                [ts - random.randint(1, 3600) for _ in range(log_count)],
                reverse=True
            )
            for j in range(log_count):
                # å¦‚æœå¯ç”¨å˜åŒ–æ¨¡å¼ä¸”æ»¡è¶³10%æ¦‚ç‡ï¼Œä½¿ç”¨æ–°è®¾å¤‡
                if use_variation and random.random() < 0.1:
                    device_info = random.choice(pg_device_data) if pg_device_data else device_info_main
                else:
                    device_info = device_info_main

                log_entry = {
                    'log_id': generate_uuid_order_id(),
                    'device': {
                        'brand': device_info['brand'],
                        'plat': device_info['plat'],
                        'platv': device_info['platv'],
                        'softv': device_info['softv'],
                        'uname': device_info['uname'],
                        'userkey': device_info['userkey'],
                        'device': device_info['device']
                    },
                    'gis': {
                        'ip': device_info['ip']
                    },
                    'network': {
                        'net': device_info['net']
                    },
                    'opa': device_info['opa'],
                    'log_type': log_types[j],
                    'ts': log_timestamps[j],  # æ—¥å¿—æ—¶é—´æˆ³
                    'product_id': product_id,  # å…³è”çš„å•†å“ID
                    'order_id': order_info['order_id'],  # å…³è”çš„è®¢å•ID
                    'user_id': order_info['user_id']
                }
                all_logs_list.append(log_entry)
        write_orders_to_sqlserver(order_list)
        # FIXME è¯¥å¤„ä»£ç åæœŸè°ƒè¯•å®Œæˆå è¯¥æ•°æ®å†™å…¥æ•°æ®åº“å’Œkafkaä¸­
        for log in all_logs_list:
            if log['log_type'] == 'search':
                log['keywords'] = generate_random_list(search_list)
        transformed_data = []
        sink_fields = ['log_id', 'log']
        for item in all_logs_list:
            log_id = item['log_id']
            data = {
                'log_id': log_id,
                'log': item
            }
            transformed_data.append(data)
        sink_dict_2postgresql(sink_data=transformed_data, table_name='logs_user_info_message', sink_fields=sink_fields)
        sink_dict_data_2kafka(sink_topic=kafka_log_topic, sink_data=all_logs_list)
        # return order_list

    if data_model == 'stream':

        all_logs_list = []

        while True:
            user_info_data = get_postgres_data(query=user_query_sql, return_df=False)
            selected_users = random.choices(user_info_data, k=min(30, len(user_info_data))) if user_info_data else []
            lst = get_postgres_data(query=query_sql, return_df=False)
            res = []
            for i, item in enumerate(lst):
                tm.sleep(2)
                user = random.choice(selected_users) if selected_users else {
                    "user_id": "default_user",
                    "uname": "åŒ¿åç”¨æˆ·",
                    "phone_num": "00000000000",
                    "address": "æœªçŸ¥åœ°å€"
                }

                color_list = item['color_list']
                # color
                if color_list is not None and len(color_list) > 0:
                    color = random.choices(color_list)[0]
                else:
                    color = '/'
                size_list = item['size_list']
                if size_list is not None and len(size_list) > 0:
                    size = random.choices(size_list)[0]
                else:
                    size = 'å‡ç '

                sale_num = generate_random_in_range(1, 9, data_type="int")

                product_link = item['product_link']
                product_id = item['product_id']
                item_id = item['item_id']
                material = item['material']
                sale_amount = item.get("sale_amount")
                product_name = item['product_desc']
                is_online_sales = item['is_online_sales']
                recommendations_product_ids = item['recommendations_product_id']

                ds_now = datetime.now()

                # 5%çš„æ¦‚ç‡ç”Ÿæˆå†å²æ—¶é—´æˆ³
                if random.random() < 0.05:
                    # è®¡ç®—æ—¶é—´èŒƒå›´ï¼šå‰3å¤©åˆ°ä»Šå¤©
                    start_time = ds_now - timedelta(days=3)

                    # ç”Ÿæˆè¯¥èŒƒå›´å†…çš„éšæœºæ—¶é—´æˆ³ï¼ˆç¡®ä¿ä¸è¶…è¿‡å½“å‰æ—¶é—´ï¼‰
                    random_timestamp = random.uniform(
                        start_time.timestamp(),
                        ds_now.timestamp()
                    )
                    ds_now = datetime.fromtimestamp(random_timestamp)
                else:
                    # 95%æ¦‚ç‡ä½¿ç”¨å½“å‰æ—¶é—´
                    ds_now = ds_now

                order_info = {
                    "order_id": generate_uuid_order_id(),
                    "user_id": user["user_id"],
                    "user_name": user["uname"],
                    "phone_number": user["phone_num"],
                    "product_link": product_link,
                    "product_id": product_id,
                    "color": color,
                    "size": size,
                    "item_id": item_id,
                    "material": material,
                    "sale_num": sale_num,
                    "sale_amount": sale_amount,
                    "total_amount": sale_num * sale_amount,
                    "product_name": product_name,
                    "is_online_sales": is_online_sales,
                    "shipping_address": user["address"],
                    "recommendations_product_ids": recommendations_product_ids,
                    "ds": ds_now.strftime('%Y-%m-%d %H:%M:%S'),
                    "ts": ds_now.timestamp() * 1000,
                }
                res.append(order_info)

                device_info_main = random.choice(pg_device_data) if pg_device_data else {
                    'brand': 'Unknown',
                    'plat': 'Unknown',
                    'platv': 'Unknown',
                    'softv': 'Unknown',
                    'uname': 'Unknown',
                    'userkey': 'Unknown',
                    'device': 'Unknown',
                    'ip': '0.0.0.0',
                    'net': 'Unknown',
                    'opa': 'Unknown'
                }
                use_variation = random.random() < user_device_change_rate
                log_count = random.randint(2, 6)
                required_logs = ['payment']
                other_logs = random.choices(
                    [log for log in log_web_page if log != 'payment'],
                    k=log_count - 1
                )
                log_types = required_logs + other_logs
                random.shuffle(log_types)
                log_timestamps = sorted(
                    [order_info["ts"] - random.randint(1, 3600) for _ in range(log_count)],
                    reverse=True
                )
                for j in range(log_count):
                    if use_variation and random.random() < 0.1:
                        device_info = random.choice(pg_device_data) if pg_device_data else device_info_main
                    else:
                        device_info = device_info_main

                    log_entry = {
                        'log_id': generate_uuid_order_id(),
                        'device': {
                            'brand': device_info['brand'],
                            'plat': device_info['plat'],
                            'platv': device_info['platv'],
                            'softv': device_info['softv'],
                            'uname': device_info['uname'],
                            'userkey': device_info['userkey'],
                            'device': device_info['device']
                        },
                        'gis': {'ip': device_info['ip']},
                        'network': {'net': device_info['net']},
                        'opa': device_info['opa'],
                        'log_type': log_types[j],
                        'ts': log_timestamps[j],
                        'product_id': order_info["product_id"],
                        'order_id': order_info["order_id"],
                        'user_id': order_info["user_id"]
                    }

                    # å¦‚æœæ˜¯æœç´¢æ—¥å¿—ï¼Œæ·»åŠ å…³é”®è¯
                    if log_types[j] == 'search':
                        log_entry['keywords'] = generate_random_list(search_list)

                    all_logs_list.append(log_entry)

                # æ¯20ä¸ªè®¢å•æ‰¹é‡å¤„ç†ä¸€æ¬¡
                if len(res) >= 20:
                    write_orders_to_sqlserver(res)

                    # å†™å…¥æ—¥å¿—åˆ°PostgreSQLå’ŒKafka
                    if all_logs_list:
                        # è½¬æ¢æ—¥å¿—æ ¼å¼
                        transformed_data = []
                        sink_fields = ['log_id', 'log']
                        for log in all_logs_list:
                            data = {
                                'log_id': log['log_id'],
                                'log': log
                            }
                            transformed_data.append(data)
                        # å†™å…¥PostgreSQL
                        sink_dict_2postgresql(sink_data=transformed_data, table_name='logs_user_info_message', sink_fields=sink_fields)
                        # å†™å…¥Kafka
                        sink_dict_data_2kafka(sink_topic=kafka_log_topic, sink_data=all_logs_list)
                        # æ¸…ç©ºæ—¥å¿—åˆ—è¡¨
                        all_logs_list.clear()
                    # æ¸…ç©ºè®¢å•åˆ—è¡¨
                    res.clear()
                print(order_info)


def main(ds_ms):
    # collect_minio_user_device_2postgresql()
    func_list = [
        {'func_name': business_mock_data, 'func_args': (ds_ms, 'batch')},
        {'func_name': business_mock_data, 'func_args': (ds_ms, 'stream')}
    ]
    process_thread_func(func_list, v_type='process', v_process_cnt=2)

    # business_mock_data(ds=ds_ms, data_model='batch')


if __name__ == '__main__':
    main(ds_ms=20251025)
    # print(generate_random_in_range(1, 5, data_type="int"))
    # collect_minio_user_device_2postgresql()
    # print(generate_uuid_order_id())
    pass
