# py_sqlserver2_enhanced.py
import logging
import os
import random
import re
import time
import warnings
from typing import Dict, List, Set, Any

import pymssql
import requests
from dotenv import load_dotenv

# å¯¼å…¥jieba
try:
    import jieba
    import jieba.posseg as pseg
    JIEBA_AVAILABLE = True
    logger_extra = "âœ… jiebaå¯ç”¨"
except ImportError:
    JIEBA_AVAILABLE = False
    logger_extra = "âŒ jiebaä¸å¯ç”¨"

# è¿‡æ»¤è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, message='pandas only supports SQLAlchemy connectable')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv()

# æ•°æ®åº“é…ç½®
mysql_ip = os.getenv('sqlserver_ip')
mysql_port = os.getenv('sqlserver_port')
mysql_user_name = os.getenv('sqlserver_user_name')
mysql_user_pwd = os.getenv('sqlserver_user_pwd')
mysql_order_db = os.getenv('sqlserver_db')

# ç¡…åŸºæµåŠ¨APIé…ç½®
SILICONFLOW_API_KEY = 'sk-vwhxpdbxdpnyozrgvbhdpoukpwasgbhscqnmlfezmwhlkkcb'
SILICONFLOW_API_URL = "https://api.siliconflow.cn/v1/chat/completions"
MODEL_NAME = "Qwen/Qwen3-8B"

class IKAnalyzer:
    """IKåˆ†è¯å™¨ - åŸºäºjiebaçš„ç»†ç²’åº¦åˆ†è¯"""

    def __init__(self):
        self.initialized = False
        if JIEBA_AVAILABLE:
            try:
                # åˆå§‹åŒ–jieba
                jieba.initialize()

                # è®¾ç½®ç»†ç²’åº¦åˆ†è¯æ¨¡å¼
                self._setup_fine_grained_mode()

                # åŠ è½½æ•æ„Ÿè¯è‡ªå®šä¹‰è¯å…¸
                self._load_sensitive_dict()

                self.initialized = True
                logger.info("ğŸ¯ IKåˆ†è¯å™¨åˆå§‹åŒ–æˆåŠŸ - ç»†ç²’åº¦æ¨¡å¼å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"IKåˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.initialized = False
        else:
            logger.warning("âŒ jiebaä¸å¯ç”¨ï¼ŒIKåˆ†è¯å™¨æ— æ³•åˆå§‹åŒ–")

    def _setup_fine_grained_mode(self):
        """è®¾ç½®ç»†ç²’åº¦åˆ†è¯æ¨¡å¼"""
        # å¼ºåˆ¶åˆ‡åˆ†æ•æ„Ÿè¯ç»„åˆ
        fine_grained_words = [
            ('å‚»', 'é€¼'), ('è‰', 'æ³¥', 'é©¬'), ('å°¼', 'ç›'), ('æ³•', 'å…‹'),
            ('åœ°', 'åŸŸ', 'é»‘'), ('åœ°', 'å›¾', 'ç‚®'), ('ç™½', 'çš®', 'çŒª'),
            ('æ­»', 'å…¨', 'å®¶'), ('ç‹—', 'æ—¥', 'çš„'), ('ç‹', 'å…«', 'è›‹')
        ]

        for word_parts in fine_grained_words:
            jieba.suggest_freq(word_parts, True)

    def _load_sensitive_dict(self):
        """åŠ è½½æ•æ„Ÿè¯è‡ªå®šä¹‰è¯å…¸"""
        sensitive_words = [
            # P0çº§åˆ«æ•æ„Ÿè¯
            'æ¯’å“', 'æªæ”¯', 'çˆ†ç‚¸', 'ææ€–', 'åˆ†è£‚', 'é‚ªæ•™', 'è¯ˆéª—', 'èµŒåš', 'è‰²æƒ…',
            # P1çº§åˆ«æ•æ„Ÿè¯
            'å‚»é€¼', 'æ“ä½ ', 'å¦ˆçš„', 'å°¼ç›', 'è‰æ³¥é©¬', 'æ³•å…‹', 'åºŸç‰©', 'åƒåœ¾', 'è ¢è´§', 'ç™½ç—´',
            'åœ°åŸŸé»‘', 'åœ°å›¾ç‚®', 'å°¼å“¥', 'é»‘é¬¼', 'ç™½çš®çŒª', 'æˆ‘æ“', 'çœŸä»–å¦ˆ',
            # P2çº§åˆ«æ•æ„Ÿè¯
            'æ­»å…¨å®¶', 'å»æ­»', 'ç‹å…«è›‹', 'ç‹—æ—¥çš„', 'æ»šè›‹', 'ç‰¹ä¹ˆçš„', 'é ', 'æ—¥', 'è‰'
        ]

        for word in sensitive_words:
            jieba.add_word(word, freq=100000, tag='sensitive')

    def fine_grained_cut(self, text: str) -> List[str]:
        """ç»†ç²’åº¦åˆ†è¯ - ç±»ä¼¼IKåˆ†è¯å™¨çš„æ•ˆæœ"""
        if not text or not isinstance(text, str):
            return []

        if not self.initialized:
            return self._simple_cut(text)

        try:
            # ç¬¬ä¸€æ­¥ï¼šç²¾ç¡®æ¨¡å¼åˆ†è¯
            words = list(jieba.cut(text, cut_all=False))
            result = []

            for word in words:
                # å¯¹é•¿è¯è¿›è¡Œè¿›ä¸€æ­¥ç»†åˆ†
                if len(word) > 2:
                    sub_words = self._check_further_cut(word)
                    result.extend(sub_words)
                else:
                    result.append(word)

            # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨æœç´¢å¼•æ“æ¨¡å¼è¿›è¡Œè¡¥å……åˆ‡åˆ†
            search_words = list(jieba.cut_for_search(text))
            if len(search_words) > len(result):
                # å¦‚æœæœç´¢å¼•æ“æ¨¡å¼åˆ‡åˆ†æ›´ç»†ï¼Œä½¿ç”¨å…¶ç»“æœ
                result = search_words

            logger.debug(f"ğŸ” IKåˆ†è¯ç»“æœ: {result}")
            return result

        except Exception as e:
            logger.warning(f"IKåˆ†è¯å¤±è´¥: {e}")
            return self._simple_cut(text)

    def _check_further_cut(self, word: str) -> List[str]:
        """æ£€æŸ¥é•¿è¯æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†"""
        # æ•æ„Ÿè¯ç»„ä»¶åº“
        sensitive_components = {
            'å‚»', 'é€¼', 'æ“', 'å°¼', 'ç›', 'è‰', 'æ³¥', 'é©¬', 'æ³•', 'å…‹',
            'åƒ', 'åœ¾', 'åºŸ', 'ç‰©', 'è ¢', 'è´§', 'ç™½', 'ç—´', 'é»‘', 'é¬¼',
            'ç‹—', 'æ—¥', 'ç‹', 'å…«', 'è›‹', 'æ»š', 'è›‹'
        }

        # å¦‚æœåŒ…å«æ•æ„Ÿè¯æˆåˆ†ï¼Œå°è¯•è¿›ä¸€æ­¥åˆ‡åˆ†
        if any(comp in word for comp in sensitive_components):
            try:
                # ä½¿ç”¨æœç´¢å¼•æ“æ¨¡å¼è¿›è¡Œæ›´ç»†ç²’åº¦çš„åˆ‡åˆ†
                fine_words = list(jieba.cut_for_search(word))
                if len(fine_words) > 1:
                    return fine_words
            except:
                pass

        return [word]

    def _simple_cut(self, text: str) -> List[str]:
        """ç®€å•åˆ†è¯å›é€€"""
        delimiters = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€ˆã€‰ã€\n\t ,.!?;:"''()[]{}'
        pattern = f'([{re.escape(delimiters)}])'
        segments = re.split(pattern, text)
        return [seg.strip() for seg in segments if seg.strip()]

    def analyze_text_components(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬æˆåˆ†"""
        words = self.fine_grained_cut(text)

        return {
            'words': words,
            'word_count': len(words),
            'char_count': len(text),
            'unique_words': list(set(words)),
            'word_length_distribution': [len(word) for word in words],
            'analyzer_type': 'ik_analyzer'
        }

# åˆå§‹åŒ–IKåˆ†è¯å™¨
ik_analyzer = IKAnalyzer()

class EnhancedSensitiveWordClassifier:
    """å¢å¼ºç‰ˆæ•æ„Ÿè¯åˆ†ç±»å™¨ - é›†æˆIKåˆ†è¯å™¨"""

    def __init__(self, use_ik_analyzer: bool = True):
        self.p0_words: Set[str] = set()
        self.p1_words: Set[str] = set()
        self.p2_words: Set[str] = set()
        self._word_categories: Dict[str, str] = {}
        self.use_ik_analyzer = use_ik_analyzer
        self.ik_analyzer = ik_analyzer

        # åŸºç¡€æ•æ„Ÿè¯åº“
        self.base_sensitive_words = {
            "p0": {"æ¯’å“", "æªæ”¯", "çˆ†ç‚¸", "ææ€–", "åˆ†è£‚", "é‚ªæ•™", "è¯ˆéª—", "èµŒåš", "è‰²æƒ…"},
            "p1": {"å‚»é€¼", "æ“ä½ ", "å¦ˆçš„", "å°¼ç›", "è‰æ³¥é©¬", "æ³•å…‹", "åºŸç‰©", "åƒåœ¾", "è ¢è´§", "ç™½ç—´"},
            "p2": {"åœ°åŸŸé»‘", "åœ°å›¾ç‚®", "æ­»å…¨å®¶", "å»æ­»", "ç™½çš®çŒª", "é»‘é¬¼"}
        }

        # æ„å»ºæ•æ„Ÿè¯å˜ä½“åº“
        self._build_sensitive_variants()

    def _build_sensitive_variants(self):
        """æ„å»ºæ•æ„Ÿè¯å˜ä½“åº“"""
        self.sensitive_variants = {
            # æ‹¼éŸ³å˜ä½“
            'å‚»é€¼': ['sb', 'SB', 'Sb', 'sB', 'å‚»B', 'å‚»b'],
            'å¦ˆçš„': ['md', 'MD', 'Md', 'é©¬å¾·', 'éº»çš„'],
            'å°¼ç›': ['nm', 'NM', 'Nm', 'ä½ å¦ˆ', 'å°¼é©¬'],
            'è‰æ³¥é©¬': ['cnm', 'CNM', 'Cnm', 'è‰å°¼é©¬'],
            'æ³•å…‹': ['fk', 'FK', 'Fk'],

            # è°éŸ³å˜ä½“
            'å‚»é€¼': ['æ²™æ¯”', 'ç…ç¬”', 'å‚»æ¯”', 'å•¥æ¯”'],
            'åƒåœ¾': ['æ‹‰åŸº', 'è¾£é¸¡', 'åƒé¸¡'],

            # æ‹†å­—å˜ä½“
            'å‚»é€¼': ['å‚» é€¼', 'å‚»-é€¼', 'å‚»_é€¼', 'å‚»Â·é€¼'],
            'è‰æ³¥é©¬': ['è‰ æ³¥ é©¬', 'è‰-æ³¥-é©¬', 'è‰_æ³¥_é©¬'],
            'å°¼ç›': ['å°¼ ç›', 'å°¼-ç›', 'å°¼_ç›'],
        }

    def load_sensitive_words(self, custom_file_path: str = None) -> bool:
        """åŠ è½½å¹¶åˆ†ç±»æ•æ„Ÿè¯"""
        try:
            file_path = self._find_sensitive_words_file(custom_file_path)
            file_words = []

            if file_path:
                file_words = self._read_words_from_file(file_path)
                logger.info(f"ä»æ–‡ä»¶åŠ è½½äº† {len(file_words)} ä¸ªæ•æ„Ÿè¯")

            # åˆå¹¶åŸºç¡€è¯åº“å’Œæ–‡ä»¶è¯åº“
            all_words = self._merge_word_lists(file_words)
            self._classify_words(all_words)

            logger.info(f"æ•æ„Ÿè¯åˆ†ç±»å®Œæˆ - P0: {len(self.p0_words)}ä¸ª, P1: {len(self.p1_words)}ä¸ª, P2: {len(self.p2_words)}ä¸ª")
            return True

        except Exception as e:
            logger.error(f"åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸºç¡€è¯åº“")
            self._load_base_words_only()
            return False

    def _merge_word_lists(self, file_words: List[str]) -> List[str]:
        """åˆå¹¶è¯åº“"""
        all_words = set()
        for category_words in self.base_sensitive_words.values():
            all_words.update(category_words)
        all_words.update(file_words)
        logger.info(f"åˆå¹¶åæ€»è¯æ•°: {len(all_words)}")
        return list(all_words)

    def _load_base_words_only(self):
        """ä»…åŠ è½½åŸºç¡€è¯åº“"""
        all_words = set()
        for category_words in self.base_sensitive_words.values():
            all_words.update(category_words)
        self._classify_words(list(all_words))

    def _find_sensitive_words_file(self, custom_path: str = None) -> str:
        """æŸ¥æ‰¾æ•æ„Ÿè¯æ–‡ä»¶"""
        possible_paths = [
            custom_path,
            r"D:\sx1\stream_prod\stream_py\src\test\Identify-sensitive-words.txt",
            "D:/sx1/stream_prod/stream_py/src/test/Identify-sensitive-words.txt",
            "./Identify-sensitive-words.txt",
            "Identify-sensitive-words.txt",
        ]
        for file_path in possible_paths:
            if file_path and os.path.exists(file_path):
                logger.info(f"æ‰¾åˆ°æ•æ„Ÿè¯æ–‡ä»¶: {file_path}")
                return file_path
        logger.warning("æœªæ‰¾åˆ°æ•æ„Ÿè¯æ–‡ä»¶")
        return None

    def _read_words_from_file(self, file_path: str) -> List[str]:
        """ä»æ–‡ä»¶è¯»å–æ•æ„Ÿè¯"""
        words = set()
        try:
            with open(file_path, "r", encoding='utf-8') as f:
                for line in f:
                    word = line.strip()
                    if word and not word.startswith('#'):
                        if '|' in word:
                            words.update(w.strip() for w in word.split('|') if w.strip())
                        else:
                            words.add(word)
            return list(words)
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return []

    def _classify_words(self, all_words: List[str]):
        """å°†æ•æ„Ÿè¯åˆ†ç±»"""
        for word in all_words:
            category = self._determine_category(word)
            if category == "p0":
                self.p0_words.add(word)
            elif category == "p1":
                self.p1_words.add(word)
            else:
                self.p2_words.add(word)
            self._word_categories[word] = category

    def _determine_category(self, word: str) -> str:
        """ç¡®å®šæ•æ„Ÿè¯åˆ†ç±»"""
        # ä¼˜å…ˆæ£€æŸ¥åŸºç¡€è¯åº“
        if word in self.base_sensitive_words["p0"]:
            return "p0"
        elif word in self.base_sensitive_words["p1"]:
            return "p1"
        elif word in self.base_sensitive_words["p2"]:
            return "p2"

        # åŸºäºå…³é”®è¯åˆ†ç±»
        p0_keywords = ['æ¯’å“', 'æªæ”¯', 'æš´æ', 'åˆ†è£‚', 'é‚ªæ•™', 'è¯ˆéª—', 'çˆ†ç‚¸', 'ææ€–', 'èµŒåš', 'è‰²æƒ…']
        p1_keywords = ['å‚»é€¼', 'æ“ä½ ', 'å¦ˆçš„', 'åœ°åŸŸé»‘', 'åœ°å›¾ç‚®', 'å°¼å“¥', 'å°¼ç›', 'è‰æ³¥é©¬', 'æ³•å…‹', 'é»‘é¬¼', 'ç™½çš®çŒª']

        if any(keyword in word for keyword in p0_keywords):
            return "p0"
        elif any(keyword in word for keyword in p1_keywords):
            return "p1"
        elif len(word) <= 3:
            return "p1"
        else:
            return "p2"

    def check_sensitive_enhanced(self, text: str) -> Dict[str, List[str]]:
        """å¢å¼ºç‰ˆæ•æ„Ÿè¯æ£€æµ‹ - ä½¿ç”¨IKåˆ†è¯å™¨"""
        if not text or not isinstance(text, str):
            return {"p0": [], "p1": [], "p2": []}

        result = {"p0": [], "p1": [], "p2": []}
        text_lower = text.lower()

        if self.use_ik_analyzer and self.ik_analyzer.initialized:
            # ä½¿ç”¨IKåˆ†è¯å™¨è¿›è¡Œç»†ç²’åº¦åˆ†è¯
            words = self.ik_analyzer.fine_grained_cut(text)
            found_words = set()

            logger.info(f"ğŸ” IKåˆ†è¯ç»“æœ: {words}")

            # åœ¨åˆ†è¯ç»“æœä¸­æ£€æµ‹æ•æ„Ÿè¯
            for word in words:
                # æ£€æŸ¥åŸè¯
                self._check_word_sensitive(word, found_words)

                # æ£€æŸ¥å°å†™ç‰ˆæœ¬
                self._check_word_sensitive(word.lower(), found_words)

                # æ£€æŸ¥å˜ä½“
                self._check_variants(word, found_words)

            # ç›´æ¥åŒ¹é…ä½œä¸ºè¡¥å……ï¼ˆå¤„ç†IKå¯èƒ½æ¼æ‰çš„æƒ…å†µï¼‰
            for sensitive_word, category in self._word_categories.items():
                if (sensitive_word in text or
                        sensitive_word.lower() in text_lower):
                    found_words.add((sensitive_word, category))

            # æ•´ç†ç»“æœ
            for word, category in found_words:
                result[category].append(word)
        else:
            # å›é€€åˆ°æ™®é€šåŒ¹é…
            for word, category in self._word_categories.items():
                if word in text or word.lower() in text_lower:
                    result[category].append(word)

        # å»é‡
        for category in result:
            result[category] = list(set(result[category]))

        return result

    def _check_word_sensitive(self, word: str, found_words: set):
        """æ£€æŸ¥å•ä¸ªè¯æ˜¯å¦æ•æ„Ÿ"""
        for sensitive_word, category in self._word_categories.items():
            if (sensitive_word == word or
                    sensitive_word in word or
                    word in sensitive_word):
                found_words.add((sensitive_word, category))

    def _check_variants(self, word: str, found_words: set):
        """æ£€æŸ¥æ•æ„Ÿè¯å˜ä½“"""
        for sensitive_word, variants in self.sensitive_variants.items():
            if word in variants:
                category = self._word_categories.get(sensitive_word, "p2")
                found_words.add((sensitive_word, category))

    def get_detailed_analysis(self, text: str) -> Dict[str, Any]:
        """è·å–è¯¦ç»†çš„æ•æ„Ÿè¯åˆ†ææŠ¥å‘Š"""
        sensitive_result = self.check_sensitive_enhanced(text)

        if self.use_ik_analyzer and self.ik_analyzer.initialized:
            ik_analysis = self.ik_analyzer.analyze_text_components(text)
        else:
            ik_analysis = {"words": [], "word_count": 0, "char_count": len(text)}

        total_sensitive = sum(len(words) for words in sensitive_result.values())

        return {
            "sensitive_words": sensitive_result,
            "total_sensitive_count": total_sensitive,
            "has_sensitive": total_sensitive > 0,
            "text_analysis": ik_analysis,
            "detection_method": "IKåˆ†è¯å™¨" if (self.use_ik_analyzer and self.ik_analyzer.initialized) else "æ™®é€šåŒ¹é…"
        }

    def get_statistics(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "p0_count": len(self.p0_words),
            "p1_count": len(self.p1_words),
            "p2_count": len(self.p2_words),
            "total_count": len(self._word_categories)
        }

# åˆå§‹åŒ–å¢å¼ºç‰ˆæ•æ„Ÿè¯åˆ†ç±»å™¨
enhanced_classifier = EnhancedSensitiveWordClassifier(use_ik_analyzer=True)
enhanced_classifier.load_sensitive_words()

def get_random_element(lst):
    """ä»åˆ—è¡¨ä¸­éšæœºè¿”å›ä¸€ä¸ªå…ƒç´ """
    if not lst:
        return None
    return random.choice(lst)

def detect_sensitive_content_enhanced(text):
    """å¢å¼ºç‰ˆæ•æ„Ÿå†…å®¹æ£€æµ‹ - ä½¿ç”¨IKåˆ†è¯å™¨"""
    if not isinstance(text, str):
        return False, [], {}

    analysis_result = enhanced_classifier.get_detailed_analysis(text)

    # åˆå¹¶æ‰€æœ‰çº§åˆ«çš„æ•æ„Ÿè¯
    all_detected_words = []
    for category in ["p0", "p1", "p2"]:
        all_detected_words.extend(analysis_result["sensitive_words"][category])

    has_sensitive = len(all_detected_words) > 0

    return has_sensitive, all_detected_words, analysis_result

# åŸæœ‰çš„å…¶ä»–å‡½æ•°ä¿æŒä¸å˜ï¼ˆmark_violation_user, process_violation_users, fix_encoding, split_product_infoç­‰ï¼‰
# è¿™é‡Œåªå±•ç¤ºä¿®æ”¹åçš„generate_ai_reviewå‡½æ•°

def generate_ai_review(product_info, user_id, order_id, conn, max_retries=3):
    """
    ä½¿ç”¨ç¡…åŸºæµåŠ¨APIç”Ÿæˆå•†å“è¯„è®ºï¼ŒåŠ å…¥å¢å¼ºç‰ˆè¯­è¨€å’Œè„è¯è¿‡æ»¤
    """
    brand = product_info.get('brand', '')
    english_name = product_info.get('english_name', '')
    chinese_name = product_info.get('chinese_name', '')

    # éšæœºå†³å®šè¯„è®ºç±»å‹ï¼š55%å¥½è¯„ï¼Œ30%ä¸­è¯„ï¼Œ15%å·®è¯„
    random_num = random.random()

    if random_num < 0.55:
        # å¥½è¯„ 55%
        review_type = "å¥½è¯„"
        sentiment_instruction = "è¡¨è¾¾æ»¡æ„å’Œæ¨èï¼Œè¯­æ°”ç§¯ææ­£é¢"
    elif random_num < 0.85:
        # ä¸­è¯„ 30%
        review_type = "ä¸­è¯„"
        sentiment_instruction = "è¡¨è¾¾ä¸­ç«‹å®¢è§‚ï¼Œæ—¢æœ‰ä¼˜ç‚¹ä¹Ÿæœ‰ç¼ºç‚¹ï¼Œè¯­æ°”å¹³å’Œ"
    else:
        # å·®è¯„ 15%
        review_type = "å·®è¯„"
        sentiment_instruction = "è¡¨è¾¾ä¸æ»¡å’Œæ‰¹è¯„ï¼Œè¯­æ°”å¯ä»¥å¼ºçƒˆä¸€äº›"

    # è„è¯åç¼€åˆ—è¡¨
    sensitive_suffixes = [
        " å¦ˆçš„", " æ“", " å‚»é€¼", " åƒåœ¾", " ç‰¹ä¹ˆçš„", " è‰", " æ—¥", " é ",
        " å°¼ç›", " æˆ‘æ“", " çœŸä»–å¦ˆ", " ç‹—æ—¥çš„", " æ»šè›‹", " å»æ­»", " ç‹å…«è›‹"
    ]

    # æ„å»ºæç¤ºè¯
    prompt = f"""
    è¯·ä¸ºä»¥ä¸‹å•†å“ç”Ÿæˆä¸€æ¡{review_type}ï¼š
    å“ç‰Œï¼š{brand}
    äº§å“åç§°ï¼š{english_name}
    äº§å“æè¿°ï¼š{chinese_name}

    è¦æ±‚ï¼š
    1. è¯„è®ºé•¿åº¦åœ¨20-50å­—ä¹‹é—´
    2. è¯­æ°”è‡ªç„¶ï¼ŒåƒçœŸå®ç”¨æˆ·å†™çš„
    3. åŒ…å«å…·ä½“çš„ä½¿ç”¨ä½“éªŒå’Œæ„Ÿå—
    4. {sentiment_instruction}
    5. ç›´æ¥è¿”å›è¯„è®ºå†…å®¹ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜
    6. ä¸ç”¨æ€è€ƒ
    """

    headers = {
        "Authorization": f"Bearer {SILICONFLOW_API_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "max_tokens": 100,
        "temperature": 0.8,
        "top_p": 0.8,
        "stream": False
    }

    for attempt in range(max_retries):
        try:
            logger.info(f"æ­£åœ¨è°ƒç”¨AIç”Ÿæˆ{review_type}... (å°è¯• {attempt + 1})")
            response = requests.post(SILICONFLOW_API_URL, headers=headers,
                                     json=payload, timeout=60)

            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    review = result['choices'][0]['message']['content'].strip()

                    # ä½¿ç”¨å¢å¼ºç‰ˆæ•æ„Ÿè¯æ£€æµ‹æ–¹æ³•ï¼ˆIKåˆ†è¯å™¨ï¼‰
                    has_sensitive, detected_words, analysis_detail = detect_sensitive_content_enhanced(review)

                    logger.info(f"ğŸ” IKåˆ†è¯å™¨æ£€æµ‹æŠ¥å‘Š:")
                    logger.info(f"  - åˆ†è¯æ•°é‡: {analysis_detail['text_analysis']['word_count']}")
                    logger.info(f"  - æ£€æµ‹æ–¹æ³•: {analysis_detail['detection_method']}")

                    if has_sensitive:
                        # æ ‡è®°è¿è§„ç”¨æˆ·
                        mark_violation_user(conn, user_id, order_id, detected_words, review)
                        # æ›¿æ¢æ•æ„Ÿè¯
                        for word in detected_words:
                            review = review.replace(word, "***")
                        logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°æ•æ„Ÿè¯å¹¶å·²å¤„ç†: {detected_words}")
                        logger.info(f"  - è¯¦ç»†åˆ†æ: {analysis_detail['sensitive_words']}")

                    # ä¸ºå·®è¯„æ·»åŠ æ•æ„Ÿè¯åç¼€
                    if review_type == "å·®è¯„" and random.random() < 0.3:
                        suffix = get_random_element(sensitive_suffixes)
                        if suffix:
                            review += suffix
                            # æ£€æµ‹æ·»åŠ çš„åç¼€æ˜¯å¦åŒ…å«æ•æ„Ÿè¯
                            has_sensitive_suffix, suffix_words, suffix_analysis = detect_sensitive_content_enhanced(suffix)
                            if has_sensitive_suffix:
                                mark_violation_user(conn, user_id, order_id, suffix_words, review)
                                logger.warning(f"ğŸš¨ åç¼€åŒ…å«æ•æ„Ÿè¯ï¼Œç”¨æˆ·å·²è¢«æ ‡è®°: {suffix_words}")
                            logger.info(f"  {review_type}ç”ŸæˆæˆåŠŸï¼ˆå«æ•æ„Ÿåç¼€ï¼‰: {review}")
                    else:
                        logger.info(f"  {review_type}ç”ŸæˆæˆåŠŸ: {review}")

                    return review
                else:
                    logger.error("APIè¿”å›æ ¼å¼å¼‚å¸¸")
            else:
                logger.error(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                if response.status_code == 429:
                    logger.warning("è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…åé‡è¯•...")
                    time.sleep(10)

        except requests.exceptions.Timeout:
            logger.error("è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯•...")
        except requests.exceptions.ConnectionError:
            logger.error("è¿æ¥é”™è¯¯ï¼Œé‡è¯•...")
        except Exception as e:
            logger.error(f"ç”Ÿæˆè¯„è®ºå¼‚å¸¸: {e}")

        # é‡è¯•å‰ç­‰å¾…
        if attempt < max_retries - 1:
            wait_time = 5 * (attempt + 1)
            logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)

    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤è¯„è®º
    if random_num < 0.55:
        default_review = f"{brand}çš„{english_name}å¾ˆä¸é”™ï¼Œä½¿ç”¨ä½“éªŒå¾ˆå¥½ï¼Œå€¼å¾—æ¨èã€‚"
    elif random_num < 0.85:
        default_review = f"{brand}çš„{english_name}æ•´ä½“è¿˜å¯ä»¥ï¼Œæœ‰äº›åœ°æ–¹ä¸é”™ï¼Œä½†ä¹Ÿæœ‰ä¸€äº›å°é—®é¢˜ã€‚"
    else:
        default_review = f"{brand}çš„{english_name}è´¨é‡ä¸€èˆ¬ï¼Œæœ‰äº›å¤±æœ›ï¼Œä¸å¤ªæ¨èã€‚"
        # ä¸ºå·®è¯„é»˜è®¤è¯„è®ºæ·»åŠ æ•æ„Ÿè¯åç¼€
        if random.random() < 0.3:
            suffix = get_random_element(sensitive_suffixes)
            if suffix:
                default_review += suffix
                # æ£€æµ‹é»˜è®¤è¯„è®ºçš„åç¼€
                has_sensitive, detected_words, _ = detect_sensitive_content_enhanced(default_review)
                if has_sensitive:
                    mark_violation_user(conn, user_id, order_id, detected_words, default_review)

    logger.info(f"AIç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤{review_type}")
    return default_review

# å…¶ä»–åŸæœ‰å‡½æ•°ä¿æŒä¸å˜ï¼ˆmark_violation_user, process_violation_users, fix_encoding, split_product_info, check_and_create_table, insert_data_to_sqlserver, mainï¼‰

# åœ¨mainå‡½æ•°å¼€å¤´æ·»åŠ åˆ†ç±»å™¨ä¿¡æ¯æ˜¾ç¤º
def main():
    try:
        # è¿æ¥SQL Serveræ•°æ®åº“
        conn = pymssql.connect(
            server=mysql_ip,
            user=mysql_user_name,
            password=mysql_user_pwd,
            database=mysql_order_db,
            port=int(mysql_port) if mysql_port else 1433,
            charset='UTF-8'
        )
        logger.info("SQL Serveræ•°æ®åº“è¿æ¥æˆåŠŸ!")

        # æ˜¾ç¤ºåˆ†ç±»å™¨ä¿¡æ¯
        stats = enhanced_classifier.get_statistics()
        logger.info(f"ğŸ¯ å¢å¼ºç‰ˆæ•æ„Ÿè¯åˆ†ç±»å™¨ç»Ÿè®¡:")
        logger.info(f"  - P0ä¸¥é‡è¿è§„: {stats['p0_count']}ä¸ª")
        logger.info(f"  - P1è„è¯æ­§è§†: {stats['p1_count']}ä¸ª")
        logger.info(f"  - P2ä¸€èˆ¬æ•æ„Ÿ: {stats['p2_count']}ä¸ª")
        logger.info(f"  - æ€»è®¡: {stats['total_count']}ä¸ª")
        logger.info(f"  - æ£€æµ‹æ–¹æ³•: {'IKåˆ†è¯å™¨' if enhanced_classifier.use_ik_analyzer else 'æ™®é€šåŒ¹é…'}")
        logger.info(f"  - IKåˆ†è¯å™¨çŠ¶æ€: {'å·²å¯ç”¨' if ik_analyzer.initialized else 'æœªå¯ç”¨'}")
        logger.info(f"  - jiebaçŠ¶æ€: {logger_extra}")

        # å…¶ä½™åŸæœ‰ä»£ç ä¿æŒä¸å˜...
        # [åŸæœ‰çš„mainå‡½æ•°ä»£ç ]

    except Exception as e:
        logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()