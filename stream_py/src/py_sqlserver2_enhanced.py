import logging
import os
import random
import re
import time
import warnings
from datetime import datetime
from typing import Dict, List, Set, Any

import pymssql
import requests
from dotenv import load_dotenv

# å¯¼å…¥jieba
try:
    import jieba
    import jieba.posseg as pseg
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False

# è¿‡æ»¤è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, message='pandas only supports SQLAlchemy connectable')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv()

# æ•°æ®åº“é…ç½®
mysql_ip = os.getenv('sqlserver_ip')
mysql_port = os.getenv('sqlserver_port')
mysql_user_name = os.getenv('sqlserver_user_name')
mysql_user_pwd = os.getenv('sqlserver_user_pwd')
mysql_order_db = os.getenv('sqlserver_db')

# ç¡…åŸºæµåŠ¨APIé…ç½®
SILICONFLOW_API_KEY = 'sk-vwhxpdbxdpnyozrgvbhdpoukpwasgbhscqnmlfezmwhlkkcb'
SILICONFLOW_API_URL = "https://api.siliconflow.cn/v1/chat/completions"
MODEL_NAME = "Qwen/Qwen2-7B-Instruct"

class IKAnalyzer:
    """IKåˆ†è¯å™¨ - åŸºäºjiebaçš„ç»†ç²’åº¦åˆ†è¯"""

    def __init__(self):
        self.initialized = False
        if JIEBA_AVAILABLE:
            try:
                # åˆå§‹åŒ–jieba
                jieba.initialize()

                # è®¾ç½®ç»†ç²’åº¦åˆ†è¯æ¨¡å¼
                self._setup_fine_grained_mode()

                # åŠ è½½æ•æ„Ÿè¯è‡ªå®šä¹‰è¯å…¸
                self._load_sensitive_dict()

                self.initialized = True
                logger.info("ğŸ¯ IKåˆ†è¯å™¨åˆå§‹åŒ–æˆåŠŸ - ç»†ç²’åº¦æ¨¡å¼å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"IKåˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.initialized = False
        else:
            logger.warning("âŒ jiebaä¸å¯ç”¨ï¼ŒIKåˆ†è¯å™¨æ— æ³•åˆå§‹åŒ–")

    def _setup_fine_grained_mode(self):
        """è®¾ç½®ç»†ç²’åº¦åˆ†è¯æ¨¡å¼"""
        # å¼ºåˆ¶åˆ‡åˆ†æ•æ„Ÿè¯ç»„åˆ
        fine_grained_words = [
            ('å‚»', 'é€¼'), ('è‰', 'æ³¥', 'é©¬'), ('å°¼', 'ç›'), ('æ³•', 'å…‹'),
            ('åœ°', 'åŸŸ', 'é»‘'), ('åœ°', 'å›¾', 'ç‚®'), ('ç™½', 'çš®', 'çŒª'),
            ('æ­»', 'å…¨', 'å®¶'), ('ç‹—', 'æ—¥', 'çš„'), ('ç‹', 'å…«', 'è›‹')
        ]

        for word_parts in fine_grained_words:
            jieba.suggest_freq(word_parts, True)

    def _load_sensitive_dict(self):
        """åŠ è½½æ•æ„Ÿè¯è‡ªå®šä¹‰è¯å…¸"""
        sensitive_words = [
            # P0çº§åˆ«æ•æ„Ÿè¯
            'æ¯’å“', 'æªæ”¯', 'çˆ†ç‚¸', 'ææ€–', 'åˆ†è£‚', 'é‚ªæ•™', 'è¯ˆéª—', 'èµŒåš', 'è‰²æƒ…',
            # P1çº§åˆ«æ•æ„Ÿè¯
            'å‚»é€¼', 'æ“ä½ ', 'å¦ˆçš„', 'å°¼ç›', 'è‰æ³¥é©¬', 'æ³•å…‹', 'åºŸç‰©',  'è ¢è´§', 'ç™½ç—´',
            'åœ°åŸŸé»‘', 'åœ°å›¾ç‚®', 'å°¼å“¥', 'é»‘é¬¼', 'ç™½çš®çŒª', 'æˆ‘æ“', 'çœŸä»–å¦ˆ',
            # P2çº§åˆ«æ•æ„Ÿè¯
            'æ­»å…¨å®¶', 'å»æ­»', 'ç‹å…«è›‹', 'ç‹—æ—¥çš„', 'æ»šè›‹', 'ç‰¹ä¹ˆçš„', 'é ', 'æ—¥', 'è‰'
        ]

        for word in sensitive_words:
            jieba.add_word(word, freq=100000, tag='sensitive')

    def fine_grained_cut(self, text: str) -> List[str]:
        """ç»†ç²’åº¦åˆ†è¯ - ç±»ä¼¼IKåˆ†è¯å™¨çš„æ•ˆæœ"""
        if not text or not isinstance(text, str):
            return []

        if not self.initialized:
            return self._simple_cut(text)

        try:
            # ç¬¬ä¸€æ­¥ï¼šç²¾ç¡®æ¨¡å¼åˆ†è¯
            words = list(jieba.cut(text, cut_all=False))
            result = []

            for word in words:
                # å¯¹é•¿è¯è¿›è¡Œè¿›ä¸€æ­¥ç»†åˆ†
                if len(word) > 2:
                    sub_words = self._check_further_cut(word)
                    result.extend(sub_words)
                else:
                    result.append(word)

            # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨æœç´¢å¼•æ“æ¨¡å¼è¿›è¡Œè¡¥å……åˆ‡åˆ†
            search_words = list(jieba.cut_for_search(text))
            if len(search_words) > len(result):
                # å¦‚æœæœç´¢å¼•æ“æ¨¡å¼åˆ‡åˆ†æ›´ç»†ï¼Œä½¿ç”¨å…¶ç»“æœ
                result = search_words

            logger.debug(f"ğŸ” IKåˆ†è¯ç»“æœ: {words} -> {result}")
            return result

        except Exception as e:
            logger.warning(f"IKåˆ†è¯å¤±è´¥: {e}")
            return self._simple_cut(text)

    def _check_further_cut(self, word: str) -> List[str]:
        """æ£€æŸ¥é•¿è¯æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†"""
        # æ•æ„Ÿè¯ç»„ä»¶åº“
        sensitive_components = {
            'å‚»', 'é€¼', 'æ“', 'å°¼', 'ç›', 'è‰', 'æ³¥', 'é©¬', 'æ³•', 'å…‹',
            'åƒ', 'åœ¾', 'åºŸ', 'ç‰©', 'è ¢', 'è´§', 'ç™½', 'ç—´', 'é»‘', 'é¬¼',
            'ç‹—', 'æ—¥', 'ç‹', 'å…«', 'è›‹', 'æ»š', 'è›‹'
        }

        # å¦‚æœåŒ…å«æ•æ„Ÿè¯æˆåˆ†ï¼Œå°è¯•è¿›ä¸€æ­¥åˆ‡åˆ†
        if any(comp in word for comp in sensitive_components):
            try:
                # ä½¿ç”¨æœç´¢å¼•æ“æ¨¡å¼è¿›è¡Œæ›´ç»†ç²’åº¦çš„åˆ‡åˆ†
                fine_words = list(jieba.cut_for_search(word))
                if len(fine_words) > 1:
                    return fine_words
            except:
                pass

        return [word]

    def _simple_cut(self, text: str) -> List[str]:
        """ç®€å•åˆ†è¯å›é€€"""
        delimiters = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€ˆã€‰ã€\n\t ,.!?;:"''()[]{}'
        pattern = f'([{re.escape(delimiters)}])'
        segments = re.split(pattern, text)
        return [seg.strip() for seg in segments if seg.strip()]

    def analyze_text_components(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬æˆåˆ†"""
        words = self.fine_grained_cut(text)

        return {
            'words': words,
            'word_count': len(words),
            'char_count': len(text),
            'unique_words': list(set(words)),
            'word_length_distribution': [len(word) for word in words],
            'analyzer_type': 'ik_analyzer'
        }

# åˆå§‹åŒ–IKåˆ†è¯å™¨
ik_analyzer = IKAnalyzer()

class EnhancedSensitiveWordClassifier:
    """å¢å¼ºç‰ˆæ•æ„Ÿè¯åˆ†ç±»å™¨ - é›†æˆIKåˆ†è¯å™¨"""

    def __init__(self, use_ik_analyzer: bool = True):
        self.p0_words: Set[str] = set()
        self.p1_words: Set[str] = set()
        self.p2_words: Set[str] = set()
        self._word_categories: Dict[str, str] = {}
        self.use_ik_analyzer = use_ik_analyzer
        self.ik_analyzer = ik_analyzer

        # åŸºç¡€æ•æ„Ÿè¯åº“
        self.base_sensitive_words = {
            "p0": {"æ¯’å“", "æªæ”¯", "çˆ†ç‚¸", "ææ€–", "åˆ†è£‚", "é‚ªæ•™", "è¯ˆéª—", "èµŒåš", "è‰²æƒ…"},
            "p1": {"å‚»é€¼", "æ“ä½ ", "å¦ˆçš„", "å°¼ç›", "è‰æ³¥é©¬", "æ³•å…‹", "åºŸç‰©", "è ¢è´§", "ç™½ç—´", "æˆ‘æ“", "çœŸä»–å¦ˆ", "ç‹—æ—¥çš„", "çƒ‚è´§"},  # æ·»åŠ çƒ‚è´§
            "p2": {"åœ°åŸŸé»‘", "åœ°å›¾ç‚®", "æ­»å…¨å®¶", "å»æ­»", "ç™½çš®çŒª", "é»‘é¬¼", "æ»šè›‹", "ç‹å…«è›‹", "ç‰¹ä¹ˆçš„", "é ", "æ—¥", "è‰"}
        }

        # æ„å»ºæ•æ„Ÿè¯å˜ä½“åº“
        self._build_sensitive_variants()

    def _build_sensitive_variants(self):
        """æ„å»ºæ•æ„Ÿè¯å˜ä½“åº“"""
        self.sensitive_variants = {
            # æ‹¼éŸ³å˜ä½“
            'å‚»é€¼': ['sb', 'SB', 'Sb', 'sB', 'å‚»B', 'å‚»b'],
            'å¦ˆçš„': ['md', 'MD', 'Md', 'é©¬å¾·', 'éº»çš„'],
            'å°¼ç›': ['nm', 'NM', 'Nm', 'ä½ å¦ˆ', 'å°¼é©¬'],
            'è‰æ³¥é©¬': ['cnm', 'CNM', 'Cnm', 'è‰å°¼é©¬'],
            'æ³•å…‹': ['fk', 'FK', 'Fk'],

            # è°éŸ³å˜ä½“
            'å‚»é€¼': ['æ²™æ¯”', 'ç…ç¬”', 'å‚»æ¯”', 'å•¥æ¯”'],
            'åºŸç‰©': ['è´¹ç‰©', 'åºŸæ­¦'],

            # æ‹†å­—å˜ä½“
            'å‚»é€¼': ['å‚» é€¼', 'å‚»-é€¼', 'å‚»_é€¼', 'å‚»Â·é€¼'],
            'è‰æ³¥é©¬': ['è‰ æ³¥ é©¬', 'è‰-æ³¥-é©¬', 'è‰_æ³¥_é©¬'],
            'å°¼ç›': ['å°¼ ç›', 'å°¼-ç›', 'å°¼_ç›'],
        }

    def load_sensitive_words(self, custom_file_path: str = None) -> bool:
        """åŠ è½½å¹¶åˆ†ç±»æ•æ„Ÿè¯"""
        try:
            file_path = self._find_sensitive_words_file(custom_file_path)
            file_words = []

            if file_path:
                file_words = self._read_words_from_file(file_path)
                logger.info(f"ä»æ–‡ä»¶åŠ è½½äº† {len(file_words)} ä¸ªæ•æ„Ÿè¯")

            # åˆå¹¶åŸºç¡€è¯åº“å’Œæ–‡ä»¶è¯åº“
            all_words = self._merge_word_lists(file_words)
            self._classify_words(all_words)

            logger.info(f"æ•æ„Ÿè¯åˆ†ç±»å®Œæˆ - P0: {len(self.p0_words)}ä¸ª, P1: {len(self.p1_words)}ä¸ª, P2: {len(self.p2_words)}ä¸ª")
            return True

        except Exception as e:
            logger.error(f"åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸºç¡€è¯åº“")
            self._load_base_words_only()
            return False

    def _merge_word_lists(self, file_words: List[str]) -> List[str]:
        """åˆå¹¶è¯åº“"""
        all_words = set()
        for category_words in self.base_sensitive_words.values():
            all_words.update(category_words)
        all_words.update(file_words)
        logger.info(f"åˆå¹¶åæ€»è¯æ•°: {len(all_words)}")
        return list(all_words)

    def _load_base_words_only(self):
        """ä»…åŠ è½½åŸºç¡€è¯åº“"""
        all_words = set()
        for category_words in self.base_sensitive_words.values():
            all_words.update(category_words)
        self._classify_words(list(all_words))

    def _find_sensitive_words_file(self, custom_path: str = None) -> str:
        """æŸ¥æ‰¾æ•æ„Ÿè¯æ–‡ä»¶"""
        possible_paths = [
            custom_path,
            r"D:\sx1\stream_prod\stream_py\src\test\Identify-sensitive-words.txt",
            "D:/sx1/stream_prod/stream_py/src/test/Identify-sensitive-words.txt",
            "./Identify-sensitive-words.txt",
            "Identify-sensitive-words.txt",
        ]
        for file_path in possible_paths:
            if file_path and os.path.exists(file_path):
                logger.info(f"æ‰¾åˆ°æ•æ„Ÿè¯æ–‡ä»¶: {file_path}")
                return file_path
        logger.warning("æœªæ‰¾åˆ°æ•æ„Ÿè¯æ–‡ä»¶")
        return None

    def _read_words_from_file(self, file_path: str) -> List[str]:
        """ä»æ–‡ä»¶è¯»å–æ•æ„Ÿè¯"""
        words = set()
        try:
            with open(file_path, "r", encoding='utf-8') as f:
                for line in f:
                    word = line.strip()
                    if word and not word.startswith('#'):
                        if '|' in word:
                            words.update(w.strip() for w in word.split('|') if w.strip())
                        else:
                            words.add(word)
            return list(words)
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return []

    def _classify_words(self, all_words: List[str]):
        """å°†æ•æ„Ÿè¯åˆ†ç±»"""
        for word in all_words:
            category = self._determine_category(word)
            if category == "p0":
                self.p0_words.add(word)
            elif category == "p1":
                self.p1_words.add(word)
            else:
                self.p2_words.add(word)
            self._word_categories[word] = category

    def _determine_category(self, word: str) -> str:
        """ç¡®å®šæ•æ„Ÿè¯åˆ†ç±»"""
        # ä¼˜å…ˆæ£€æŸ¥åŸºç¡€è¯åº“
        if word in self.base_sensitive_words["p0"]:
            return "p0"
        elif word in self.base_sensitive_words["p1"]:
            return "p1"
        elif word in self.base_sensitive_words["p2"]:
            return "p2"

        # åŸºäºå…³é”®è¯åˆ†ç±»
        p0_keywords = ['æ¯’å“', 'æªæ”¯', 'æš´æ', 'åˆ†è£‚', 'é‚ªæ•™', 'è¯ˆéª—', 'çˆ†ç‚¸', 'ææ€–', 'èµŒåš', 'è‰²æƒ…']
        p1_keywords = ['å‚»é€¼', 'æ“ä½ ', 'å¦ˆçš„', 'åœ°åŸŸé»‘', 'åœ°å›¾ç‚®', 'å°¼å“¥', 'å°¼ç›', 'è‰æ³¥é©¬', 'æ³•å…‹', 'é»‘é¬¼', 'ç™½çš®çŒª', 'æˆ‘æ“', 'çœŸä»–å¦ˆ', 'ç‹—æ—¥çš„']
        p2_keywords = ['æ­»å…¨å®¶', 'å»æ­»', 'æ»šè›‹', 'ç‹å…«è›‹', 'ç‰¹ä¹ˆçš„', 'é ', 'æ—¥', 'è‰']

        if any(keyword in word for keyword in p0_keywords):
            return "p0"
        elif any(keyword in word for keyword in p1_keywords):
            return "p1"
        elif any(keyword in word for keyword in p2_keywords):
            return "p2"
        else:
            return "p2"

    def check_sensitive_enhanced(self, text: str) -> Dict[str, List[str]]:
        """ä¿®æ­£ç‰ˆæ•æ„Ÿè¯æ£€æµ‹ - åªæ£€æµ‹è¾“å…¥æ–‡æœ¬"""
        if not text or not isinstance(text, str):
            return {"p0": [], "p1": [], "p2": []}

        result = {"p0": [], "p1": [], "p2": []}

        if self.use_ik_analyzer and self.ik_analyzer.initialized:
            # å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯
            words = self.ik_analyzer.fine_grained_cut(text)
            found_words = set()

            # æ£€æŸ¥æ¯ä¸ªåˆ†è¯æ˜¯å¦åœ¨æ•æ„Ÿè¯åº“ä¸­
            for word in words:
                # æ£€æŸ¥åŸè¯
                if word in self._word_categories:
                    category = self._word_categories[word]
                    found_words.add((word, category))

                # æ£€æŸ¥å°å†™ç‰ˆæœ¬
                word_lower = word.lower()
                if word_lower in self._word_categories:
                    category = self._word_categories[word_lower]
                    found_words.add((word_lower, category))

                # æ£€æŸ¥å˜ä½“ - ä¼ é€’textå‚æ•°
                self._check_variants_for_word(word, found_words, text)  # ä¿®å¤ï¼šæ·»åŠ textå‚æ•°

            # æ•´ç†ç»“æœ
            for word, category in found_words:
                result[category].append(word)
        else:
            # å›é€€åˆ°æ™®é€šåŒ¹é…
            text_lower = text.lower()
            for word, category in self._word_categories.items():
                if word in text or word.lower() in text_lower:
                    result[category].append(word)

        # å»é‡
        for category in result:
            result[category] = list(set(result[category]))

        return result

    def _check_variants_for_word(self, word: str, found_words: set, text: str):
        """æ£€æŸ¥å•ä¸ªè¯çš„æ•æ„Ÿè¯å˜ä½“"""
        for sensitive_word, variants in self.sensitive_variants.items():
            # æ£€æŸ¥å½“å‰è¯æ˜¯å¦æ˜¯æ•æ„Ÿè¯çš„å˜ä½“
            if word in variants:
                category = self._word_categories.get(sensitive_word, "p2")
                found_words.add((sensitive_word, category))
            # æ£€æŸ¥æ•æ„Ÿè¯çš„å…¶ä»–å˜ä½“æ˜¯å¦åœ¨æ–‡æœ¬ä¸­
            for variant in variants:
                if variant in text and variant != word:
                    category = self._word_categories.get(sensitive_word, "p2")
                    found_words.add((sensitive_word, category))


    def get_detailed_analysis(self, text: str) -> Dict[str, Any]:
        """è·å–è¯¦ç»†çš„æ•æ„Ÿè¯åˆ†ææŠ¥å‘Š"""
        sensitive_result = self.check_sensitive_enhanced(text)

        if self.use_ik_analyzer and self.ik_analyzer.initialized:
            ik_analysis = self.ik_analyzer.analyze_text_components(text)
        else:
            ik_analysis = {"words": [], "word_count": 0, "char_count": len(text)}

        total_sensitive = sum(len(words) for words in sensitive_result.values())

        return {
            "sensitive_words": sensitive_result,
            "total_sensitive_count": total_sensitive,
            "has_sensitive": total_sensitive > 0,
            "text_analysis": ik_analysis,
            "detection_method": "IKåˆ†è¯å™¨" if (self.use_ik_analyzer and self.ik_analyzer.initialized) else "æ™®é€šåŒ¹é…"
        }

    def get_statistics(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "p0_count": len(self.p0_words),
            "p1_count": len(self.p1_words),
            "p2_count": len(self.p2_words),
            "total_count": len(self._word_categories)
        }

# åˆå§‹åŒ–å¢å¼ºç‰ˆæ•æ„Ÿè¯åˆ†ç±»å™¨
enhanced_classifier = EnhancedSensitiveWordClassifier(use_ik_analyzer=True)
enhanced_classifier.load_sensitive_words()

def detect_sensitive_content_enhanced(text):
    """ä¿®æ­£ç‰ˆæ•æ„Ÿå†…å®¹æ£€æµ‹ - åªæ£€æµ‹è¾“å…¥çš„æ–‡æœ¬"""
    if not isinstance(text, str):
        return False, [], {}

    # ä½¿ç”¨IKåˆ†è¯å™¨å¯¹è¾“å…¥çš„è¯„è®ºè¿›è¡Œåˆ†è¯
    words = ik_analyzer.fine_grained_cut(text)

    # æ£€æµ‹åˆ†è¯ç»“æœä¸­çš„æ•æ„Ÿè¯
    sensitive_result = enhanced_classifier.check_sensitive_enhanced(text)  # è¿™é‡Œä¼šè°ƒç”¨ä¿®å¤åçš„æ–¹æ³•

    # åˆå¹¶æ‰€æœ‰çº§åˆ«çš„æ•æ„Ÿè¯
    all_detected_words = []
    for category in ["p0", "p1", "p2"]:
        all_detected_words.extend(sensitive_result[category])

    has_sensitive = len(all_detected_words) > 0

    analysis_result = {
        "sensitive_words": sensitive_result,
        "total_sensitive_count": len(all_detected_words),
        "has_sensitive": has_sensitive,
        "text_analysis": {
            "words": words,
            "word_count": len(words),
            "char_count": len(text)
        },
        "detection_method": "IKåˆ†è¯å™¨"
    }

    return has_sensitive, all_detected_words, analysis_result

def mark_violation_user(conn, user_id, order_id, sensitive_words, review_content):
    """æ ‡è®°è¿è§„ç”¨æˆ·å¹¶è®°å½•åˆ°æ•°æ®åº“ - å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«æ•æ„Ÿè¯çº§åˆ«"""
    try:
        cursor = conn.cursor()

        # æ£€æŸ¥ç”¨æˆ·è¿è§„è¡¨æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»ºï¼ˆåŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µï¼‰
        check_table_sql = """
        IF OBJECT_ID('user_violation_records', 'U') IS NULL
        BEGIN
            CREATE TABLE user_violation_records (
                id INT IDENTITY(1,1) PRIMARY KEY,
                user_id NVARCHAR(100),
                order_id NVARCHAR(100),
                sensitive_words NVARCHAR(500),
                review_content NVARCHAR(1000),
                violation_level NVARCHAR(50),
                p0_count INT DEFAULT 0,
                p1_count INT DEFAULT 0,
                p2_count INT DEFAULT 0,
                handled BIT DEFAULT 0,
                created_time DATETIME DEFAULT GETDATE()
            )
            PRINT 'ç”¨æˆ·è¿è§„è®°å½•è¡¨åˆ›å»ºæˆåŠŸ: user_violation_records'
        END
        ELSE
        BEGIN
            -- è¡¨å·²å­˜åœ¨ï¼Œæ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±å­—æ®µ
            IF NOT EXISTS (SELECT 1 FROM sys.columns WHERE object_id = OBJECT_ID('user_violation_records') AND name = 'p0_count')
            BEGIN
                ALTER TABLE user_violation_records ADD p0_count INT DEFAULT 0
                PRINT 'æ·»åŠ å­—æ®µ: p0_count'
            END
            
            IF NOT EXISTS (SELECT 1 FROM sys.columns WHERE object_id = OBJECT_ID('user_violation_records') AND name = 'p1_count')
            BEGIN
                ALTER TABLE user_violation_records ADD p1_count INT DEFAULT 0
                PRINT 'æ·»åŠ å­—æ®µ: p1_count'
            END
            
            IF NOT EXISTS (SELECT 1 FROM sys.columns WHERE object_id = OBJECT_ID('user_violation_records') AND name = 'p2_count')
            BEGIN
                ALTER TABLE user_violation_records ADD p2_count INT DEFAULT 0
                PRINT 'æ·»åŠ å­—æ®µ: p2_count'
            END
            
            IF NOT EXISTS (SELECT 1 FROM sys.columns WHERE object_id = OBJECT_ID('user_violation_records') AND name = 'violation_level')
            BEGIN
                ALTER TABLE user_violation_records ADD violation_level NVARCHAR(50)
                PRINT 'æ·»åŠ å­—æ®µ: violation_level'
            END
        END
        """
        cursor.execute(check_table_sql)
        conn.commit()  # æäº¤è¡¨ç»“æ„å˜æ›´

        # åˆ†ææ•æ„Ÿè¯çº§åˆ«
        p0_count = 0
        p1_count = 0
        p2_count = 0

        for word in sensitive_words:
            category = enhanced_classifier._word_categories.get(word, "p2")
            if category == "p0":
                p0_count += 1
            elif category == "p1":
                p1_count += 1
            else:
                p2_count += 1

        # ç¡®å®šè¿è§„çº§åˆ«
        violation_level = "LOW"
        if p0_count > 0:
            violation_level = "CRITICAL"
        elif p1_count >= 2:
            violation_level = "HIGH"
        elif p1_count >= 1 or p2_count >= 3:
            violation_level = "MEDIUM"

        # æ’å…¥è¿è§„è®°å½•
        insert_sql = """
        INSERT INTO user_violation_records 
        (user_id, order_id, sensitive_words, review_content, violation_level, p0_count, p1_count, p2_count)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """

        cursor.execute(insert_sql, (
            user_id,
            order_id,
            ','.join(sensitive_words),
            review_content,
            violation_level,
            p0_count,
            p1_count,
            p2_count
        ))

        conn.commit()
        logger.warning(f"ç”¨æˆ· {user_id} å› ä½¿ç”¨æ•æ„Ÿè¯è¢«æ ‡è®°ä¸ºè¿è§„ï¼Œçº§åˆ«: {violation_level}")
        return True

    except Exception as e:
        logger.error(f"æ ‡è®°è¿è§„ç”¨æˆ·å¤±è´¥: {e}")
        conn.rollback()
        return False

def process_violation_users(conn):
    """å¤„ç†è¿è§„ç”¨æˆ·ï¼Œå¹¶æ›´æ–°oms_order_dtl_enhanced2è¡¨ä¸­çš„violation_handledå­—æ®µ"""
    try:
        cursor = conn.cursor()

        # è·å–æœªå¤„ç†çš„é«˜é£é™©è¿è§„ç”¨æˆ·
        query_sql = """
        SELECT user_id, order_id, violation_level, sensitive_words, p0_count, p1_count, p2_count, created_time
        FROM user_violation_records 
        WHERE handled = 0 AND violation_level IN ('CRITICAL', 'HIGH', 'MEDIUM')
        ORDER BY created_time DESC
        """

        cursor.execute(query_sql)
        violations = cursor.fetchall()

        for violation in violations:
            user_id, order_id, level, words, p0_count, p1_count, p2_count, create_time = violation
            logger.info(f"å¤„ç†è¿è§„ç”¨æˆ·: {user_id}, è®¢å•: {order_id}, çº§åˆ«: {level}, P0: {p0_count}, P1: {p1_count}, P2: {p2_count}, æ•æ„Ÿè¯: {words}")

            # æ›´æ–°oms_order_dtl_enhanced2è¡¨ä¸­çš„violation_handledå­—æ®µ
            update_oms_sql = """
            UPDATE oms_order_dtl_enhanced2 
            SET violation_handled = 1 
            WHERE user_id = %s AND order_id = %s AND has_sensitive_content = 1
            """
            cursor.execute(update_oms_sql, (user_id, order_id))

            updated_count = cursor.rowcount
            if updated_count > 0:
                logger.info(f"  å·²æ›´æ–° {updated_count} æ¡è®°å½•åœ¨oms_order_dtl_enhanced2è¡¨ä¸­çš„violation_handledå­—æ®µ")

            # æ ‡è®°user_violation_recordsä¸ºå·²å¤„ç†
            update_violation_sql = "UPDATE user_violation_records SET handled = 1 WHERE user_id = %s AND order_id = %s AND handled = 0"
            cursor.execute(update_violation_sql, (user_id, order_id))

        conn.commit()
        logger.info(f"å·²å¤„ç† {len(violations)} ä¸ªè¿è§„ç”¨æˆ·ï¼Œå¹¶æ›´æ–°äº†oms_order_dtl_enhanced2è¡¨ä¸­çš„violation_handledå­—æ®µ")

    except Exception as e:
        logger.error(f"å¤„ç†è¿è§„ç”¨æˆ·å¤±è´¥: {e}")
        conn.rollback()

def fix_encoding(text):
    """ä¿®å¤ç¼–ç é—®é¢˜"""
    if not isinstance(text, str):
        return text

    encodings_to_try = [
        ('latin-1', 'utf-8'),
        ('latin-1', 'gbk'),
        ('latin-1', 'gb2312'),
        ('utf-8', 'utf-8'),
    ]

    for src_enc, dst_enc in encodings_to_try:
        try:
            return text.encode(src_enc).decode(dst_enc)
        except (UnicodeEncodeError, UnicodeDecodeError):
            continue
    return text

def split_product_info(text):
    """æ ¹æ®ä¸¨å’Œæ±‰å­—æ‹†åˆ†äº§å“ä¿¡æ¯"""
    if not isinstance(text, str) or not text.strip():
        return {'brand': '', 'english_name': '', 'chinese_name': '', 'full_text': text}

    cleaned_text = fix_encoding(text)
    pattern = r'^([^ä¸¨]+)ä¸¨([^\u4e00-\u9fa5]*)([\u4e00-\u9fa5].*)$'

    match = re.match(pattern, cleaned_text)
    if match:
        brand = match.group(1).strip()
        english_part = match.group(2).strip()
        chinese_part = match.group(3).strip()

        return {
            'brand': brand,
            'english_name': english_part,
            'chinese_name': chinese_part,
            'full_text': cleaned_text
        }
    else:
        chinese_chars = re.findall(r'[\u4e00-\u9fa5]', cleaned_text)
        if chinese_chars:
            first_chinese_index = cleaned_text.find(chinese_chars[0])
            return {
                'brand': '',
                'english_name': cleaned_text[:first_chinese_index].strip(),
                'chinese_name': cleaned_text[first_chinese_index:].strip(),
                'full_text': cleaned_text
            }
        else:
            return {
                'brand': '',
                'english_name': cleaned_text,
                'chinese_name': '',
                'full_text': cleaned_text
            }

def generate_ai_review(product_info, user_id, order_id, conn, max_retries=3):
    """
    ä½¿ç”¨ç¡…åŸºæµåŠ¨APIç”Ÿæˆå•†å“è¯„è®ºï¼Œå·®è¯„æœ‰50%æ¦‚ç‡åŒ…å«æ•æ„Ÿè¯
    è¿”å›è¯„è®ºå†…å®¹å’Œæ•æ„Ÿè¯æ£€æµ‹ç»“æœ
    """
    brand = product_info.get('brand', '')
    english_name = product_info.get('english_name', '')
    chinese_name = product_info.get('chinese_name', '')

    # éšæœºå†³å®šè¯„è®ºç±»å‹ï¼š50%å¥½è¯„ï¼Œ30%ä¸­è¯„ï¼Œ20%å·®è¯„
    random_num = random.random()

    if random_num < 0.50:
        # å¥½è¯„ 50%
        review_type = "å¥½è¯„"
        sentiment_instruction = "è¡¨è¾¾æ»¡æ„å’Œæ¨èï¼Œè¯­æ°”ç§¯ææ­£é¢"
        include_sensitive = False
    elif random_num < 0.80:
        # ä¸­è¯„ 30%
        review_type = "ä¸­è¯„"
        sentiment_instruction = "è¡¨è¾¾ä¸­ç«‹å®¢è§‚ï¼Œæ—¢æœ‰ä¼˜ç‚¹ä¹Ÿæœ‰ç¼ºç‚¹ï¼Œè¯­æ°”å¹³å’Œ"
        include_sensitive = False
    else:
        # å·®è¯„ 20%
        review_type = "å·®è¯„"
        # å·®è¯„æœ‰35%æ¦‚ç‡åŒ…å«æ•æ„Ÿè¯
        include_sensitive = random.random() < 0.35
        if include_sensitive:
            sentiment_instruction = "è¡¨è¾¾æåº¦æ„¤æ€’å’Œä¸æ»¡ï¼Œä½¿ç”¨å¼ºçƒˆçš„æƒ…ç»ªåŒ–è¯­è¨€ï¼Œå¯ä»¥åŒ…å«ä¸€äº›ç½‘ç»œç”¨è¯­å’Œæƒ…ç»ªå‘æ³„"
            logger.info(f"ğŸ¯ æœ¬æ¬¡å·®è¯„å°†å°è¯•ç”ŸæˆåŒ…å«æ•æ„Ÿè¯çš„å†…å®¹")
        else:
            sentiment_instruction = "è¡¨è¾¾ä¸æ»¡å’Œæ‰¹è¯„ï¼Œè¯­æ°”å¼ºçƒˆ"

    # æ„å»ºæç¤ºè¯ - å¯¹äºåŒ…å«æ•æ„Ÿè¯çš„å·®è¯„ï¼Œä½¿ç”¨æ›´ç›´æ¥çš„æç¤º
    if include_sensitive and review_type == "å·®è¯„":
        prompt = f"""
        è¯·ä¸ºä»¥ä¸‹å•†å“ç”Ÿæˆä¸€æ¡æåº¦æ„¤æ€’çš„å·®è¯„ï¼Œè¡¨è¾¾å¼ºçƒˆä¸æ»¡ï¼š
        å“ç‰Œï¼š{brand}
        äº§å“åç§°ï¼š{english_name}
        äº§å“æè¿°ï¼š{chinese_name}

        è¦æ±‚ï¼š
        1. è¯„è®ºé•¿åº¦åœ¨15-40å­—ä¹‹é—´
        2. è¯­æ°”æå…¶æ„¤æ€’ï¼ŒåƒçœŸå®ç”¨æˆ·åœ¨å‘æ³„æƒ…ç»ª
        3. ä½¿ç”¨å¼ºçƒˆçš„æƒ…ç»ªåŒ–è¡¨è¾¾ï¼Œå¯ä»¥é€‚å½“ä½¿ç”¨ç½‘ç»œç”¨è¯­
        4. è¡¨è¾¾äº§å“è´¨é‡æå·®ã€æœåŠ¡ç³Ÿç³•ã€å®Œå…¨ä¸å€¼å¾—è´­ä¹°
        5. ç›´æ¥è¿”å›è¯„è®ºå†…å®¹ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜
        6. ä¸ç”¨æ€è€ƒï¼Œç›´æ¥è¡¨è¾¾æ„¤æ€’
        """
    else:
        prompt = f"""
        è¯·ä¸ºä»¥ä¸‹å•†å“ç”Ÿæˆä¸€æ¡{review_type}ï¼š
        å“ç‰Œï¼š{brand}
        äº§å“åç§°ï¼š{english_name}
        äº§å“æè¿°ï¼š{chinese_name}

        è¦æ±‚ï¼š
        1. è¯„è®ºé•¿åº¦åœ¨20-50å­—ä¹‹é—´
        2. è¯­æ°”è‡ªç„¶ï¼ŒåƒçœŸå®ç”¨æˆ·å†™çš„
        3. åŒ…å«å…·ä½“çš„ä½¿ç”¨ä½“éªŒå’Œæ„Ÿå—
        4. {sentiment_instruction}
        5. ç›´æ¥è¿”å›è¯„è®ºå†…å®¹ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜
        6. ä¸ç”¨æ€è€ƒ
        """

    headers = {
        "Authorization": f"Bearer {SILICONFLOW_API_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "max_tokens": 100,
        "temperature": 0.9 if include_sensitive else 0.8,  # åŒ…å«æ•æ„Ÿè¯æ—¶å¢åŠ éšæœºæ€§
        "top_p": 0.9 if include_sensitive else 0.8,
        "stream": False
    }

    for attempt in range(max_retries):
        try:
            logger.info(f"æ­£åœ¨è°ƒç”¨AIç”Ÿæˆ{review_type}... (å°è¯• {attempt + 1})")
            response = requests.post(SILICONFLOW_API_URL, headers=headers,
                                     json=payload, timeout=60)

            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    review = result['choices'][0]['message']['content'].strip()

                    # ä½¿ç”¨ä¿®æ­£ç‰ˆæ•æ„Ÿè¯æ£€æµ‹æ–¹æ³•ï¼ˆIKåˆ†è¯å™¨ï¼‰- åªæ£€æµ‹ç”Ÿæˆçš„è¯„è®º
                    has_sensitive, detected_words, analysis_detail = detect_sensitive_content_enhanced(review)

                    logger.info(f"ğŸ” IKåˆ†è¯å™¨æ£€æµ‹æŠ¥å‘Š:")
                    logger.info(f"  - åŸå§‹è¯„è®º: {review}")
                    logger.info(f"  - åˆ†è¯æ•°é‡: {analysis_detail['text_analysis']['word_count']}")
                    logger.info(f"  - æ£€æµ‹æ–¹æ³•: {analysis_detail['detection_method']}")
                    logger.info(f"  - å‘ç°æ•æ„Ÿè¯: {detected_words}")
                    logger.info(f"  - è¯¦ç»†åˆ†æ: {analysis_detail['sensitive_words']}")

                    if has_sensitive:
                        # æ ‡è®°è¿è§„ç”¨æˆ·
                        mark_violation_user(conn, user_id, order_id, detected_words, review)
                        # æ›¿æ¢æ•æ„Ÿè¯
                        for word in detected_words:
                            review = review.replace(word, "***")
                        logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°æ•æ„Ÿè¯å¹¶å·²å¤„ç†: {detected_words}")

                        # è¿”å›åŒ…å«æ•æ„Ÿè¯æ£€æµ‹ç»“æœçš„æ•°æ®
                        return review, has_sensitive, detected_words
                    else:
                        if include_sensitive and review_type == "å·®è¯„":
                            logger.info("  ğŸ“ æœ¬æ¬¡å·®è¯„å°è¯•ç”Ÿæˆæ•æ„Ÿè¯ä½†æœªæˆåŠŸï¼ŒAIå¯èƒ½è¿›è¡Œäº†è‡ªæˆ‘å®¡æŸ¥")

                        # è¿”å›æ— æ•æ„Ÿè¯çš„æ•°æ®
                        return review, False, []

                    logger.info(f"  {review_type}ç”ŸæˆæˆåŠŸ: {review}")

                else:
                    logger.error("APIè¿”å›æ ¼å¼å¼‚å¸¸")
            else:
                logger.error(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                if response.status_code == 429:
                    logger.warning("è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…åé‡è¯•...")
                    time.sleep(10)

        except requests.exceptions.Timeout:
            logger.error("è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯•...")
        except requests.exceptions.ConnectionError:
            logger.error("è¿æ¥é”™è¯¯ï¼Œé‡è¯•...")
        except Exception as e:
            logger.error(f"ç”Ÿæˆè¯„è®ºå¼‚å¸¸: {e}")

        # é‡è¯•å‰ç­‰å¾…
        if attempt < max_retries - 1:
            wait_time = 5 * (attempt + 1)
            logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)

    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤è¯„è®º
    if random_num < 0.50:
        default_review = f"{brand}çš„{english_name}å¾ˆä¸é”™ï¼Œä½¿ç”¨ä½“éªŒå¾ˆå¥½ï¼Œå€¼å¾—æ¨èã€‚"
        return default_review, False, []
    elif random_num < 0.80:
        default_review = f"{brand}çš„{english_name}æ•´ä½“è¿˜å¯ä»¥ï¼Œæœ‰äº›åœ°æ–¹ä¸é”™ï¼Œä½†ä¹Ÿæœ‰ä¸€äº›å°é—®é¢˜ã€‚"
        return default_review, False, []
    else:
        # å¯¹äºå·®è¯„ï¼Œæœ‰50%æ¦‚ç‡åœ¨é»˜è®¤è¯„è®ºä¸­åŠ å…¥æ•æ„Ÿè¯
        if random.random() < 0.5:
            sensitive_words = [ "åºŸç‰©", "å‘çˆ¹", "éª—é’±"]
            sensitive_word = random.choice(sensitive_words)
            default_review = f"{brand}çš„{english_name}çœŸæ˜¯{sensitive_word}ï¼è´¨é‡å·®åˆ°çˆ†ï¼Œå®Œå…¨æµªè´¹é’±ï¼"
            # æ£€æµ‹é»˜è®¤è¯„è®ºçš„æ•æ„Ÿè¯
            has_sensitive, detected_words, _ = detect_sensitive_content_enhanced(default_review)
            if has_sensitive:
                mark_violation_user(conn, user_id, order_id, detected_words, default_review)
                return default_review, True, detected_words
            else:
                return default_review, False, []
        else:
            default_review = f"{brand}çš„{english_name}è´¨é‡ä¸€èˆ¬ï¼Œæœ‰äº›å¤±æœ›ï¼Œä¸å¤ªæ¨èã€‚"
            return default_review, False, []

    logger.info(f"AIç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤{review_type}")

def check_and_create_table(conn):
    """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºï¼Œå¦‚æœå­˜åœ¨åˆ™æ·»åŠ ç¼ºå¤±å­—æ®µï¼ˆåŒ…å«é‡‘é¢å­—æ®µï¼‰"""
    try:
        cursor = conn.cursor()

        # é¦–å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        check_table_sql = """
        IF OBJECT_ID('oms_order_dtl_enhanced2', 'U') IS NOT NULL
        BEGIN
            SELECT 1
        END
        ELSE
        BEGIN
            SELECT 0
        END
        """
        cursor.execute(check_table_sql)
        table_exists = cursor.fetchone()[0]

        if table_exists == 0:
            # è¡¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°è¡¨ï¼ˆåŒ…å«é‡‘é¢å­—æ®µï¼‰
            create_table_sql = """
            CREATE TABLE oms_order_dtl_enhanced2 (
                id INT IDENTITY(1,1) PRIMARY KEY,
                order_id NVARCHAR(100),
                user_id NVARCHAR(100),
                product_id NVARCHAR(100),
                product_name NVARCHAR(500),
                brand NVARCHAR(100),
                english_name NVARCHAR(200),
                chinese_name NVARCHAR(300),
                ai_review NVARCHAR(1000),
                has_sensitive_content BIT DEFAULT 0,
                sensitive_words NVARCHAR(500),
                violation_handled BIT DEFAULT 0,
                sale_amount DECIMAL(18,2) DEFAULT 0.00, 
                total_amount DECIMAL(18,2) DEFAULT 0.00,-- æ–°å¢ä»˜æ¬¾é‡‘é¢å­—æ®µ
                ds DATE,
                created_time DATETIME DEFAULT GETDATE()
            )
            """
            cursor.execute(create_table_sql)
            logger.info("æ–°è¡¨åˆ›å»ºæˆåŠŸ: oms_order_dtl_enhanced2ï¼ˆåŒ…å«é‡‘é¢å­—æ®µï¼‰")
        else:
            # è¡¨å·²å­˜åœ¨ï¼Œæ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±å­—æ®µ
            logger.info("è¡¨å·²å­˜åœ¨: oms_order_dtl_enhanced2ï¼Œæ£€æŸ¥å­—æ®µå®Œæ•´æ€§...")

            # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ·»åŠ 
            columns_to_check = [
                ('has_sensitive_content', 'BIT DEFAULT 0'),
                ('sensitive_words', 'NVARCHAR(500)'),
                ('violation_handled', 'BIT DEFAULT 0'),
                ('sale_amount', 'DECIMAL(18,2) DEFAULT 0.00'),  # æ–°å¢é‡‘é¢å­—æ®µæ£€æŸ¥
                ('total_amount', 'DECIMAL(18,2) DEFAULT 0.00')
            ]

            for column_name, column_type in columns_to_check:
                check_column_sql = f"""
                IF NOT EXISTS (
                    SELECT 1 FROM sys.columns 
                    WHERE object_id = OBJECT_ID('oms_order_dtl_enhanced2') 
                    AND name = '{column_name}'
                )
                BEGIN
                    ALTER TABLE oms_order_dtl_enhanced2 ADD {column_name} {column_type}
                    PRINT 'æ·»åŠ å­—æ®µ: {column_name}'
                END
                """
                cursor.execute(check_column_sql)

            logger.info("è¡¨å­—æ®µæ£€æŸ¥å®Œæˆ")

        conn.commit()

    except Exception as e:
        logger.error(f"æ£€æŸ¥/åˆ›å»ºè¡¨å¤±è´¥: {e}")
        conn.rollback()
        raise

def insert_single_record_to_sqlserver(conn, record):
    """å°†å•æ¡è®°å½•æ’å…¥åˆ°SQL Serverï¼Œé¿å…é‡å¤æ’å…¥ï¼ˆåŒ…å«é‡‘é¢å­—æ®µï¼‰"""
    try:
        cursor = conn.cursor()

        # æ’å…¥SQL - åŒ…å«violation_handledå’Œé‡‘é¢å­—æ®µ
        insert_sql = """
        INSERT INTO oms_order_dtl_enhanced2
        (order_id, user_id, product_id, product_name, brand, english_name, chinese_name, ai_review, has_sensitive_content, sensitive_words, violation_handled, sale_amount, total_amount, ds)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """

        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•æ„Ÿè¯å¹¶æ­£ç¡®è®¾ç½®å­—æ®µå€¼
            has_sensitive_content = 1 if record['has_sensitive_content'] else 0
            sensitive_words = str(record['sensitive_words']) if record['sensitive_words'] else ''

            # å¦‚æœæœ‰æ•æ„Ÿè¯ï¼Œè‡ªåŠ¨æ ‡è®°ä¸ºå·²å¤„ç†
            violation_handled = 1 if has_sensitive_content else 0  # è‡ªåŠ¨è®¾ç½®ä¸º1

            # é‡‘é¢å­—æ®µ
            sale_amount = record.get('sale_amount', 0.00)
            total_amount = record.get('total_amount', 0.00)

            # ç›´æ¥æ’å…¥ï¼Œä¾èµ–æ•°æ®åº“çš„å”¯ä¸€çº¦æŸæ¥é¿å…é‡å¤
            cursor.execute(insert_sql, (
                str(record['order_id']),
                str(record['user_id']),
                str(record['product_id']),
                str(record['product_name'])[:500],
                str(record['brand'])[:100],
                str(record['english_name'])[:200],
                str(record['chinese_name'])[:300],
                str(record['ai_review'])[:1000],
                has_sensitive_content,
                sensitive_words[:500],
                violation_handled,  # ä½¿ç”¨è‡ªåŠ¨è®¾ç½®çš„å€¼
                sale_amount,
                total_amount,
                record['ds']
            ))

            conn.commit()
            logger.info(f"âœ… æˆåŠŸæ’å…¥è®°å½• - è®¢å•: {record['order_id']}, ç”¨æˆ·: {record['user_id']}, æ•æ„Ÿè¯: {sensitive_words}, è¿è§„å¤„ç†: {violation_handled}")

            # å¦‚æœæœ‰æ•æ„Ÿè¯ï¼Œç«‹å³å¤„ç†è¿è§„ç”¨æˆ·
            if has_sensitive_content and sensitive_words:
                sensitive_word_list = sensitive_words.split(',') if ',' in sensitive_words else [sensitive_words]
                mark_violation_user(conn, record['user_id'], record['order_id'], sensitive_word_list, record['ai_review'])
                logger.info(f"ğŸš¨ è‡ªåŠ¨æ ‡è®°è¿è§„ç”¨æˆ·: {record['user_id']}, æ•æ„Ÿè¯: {sensitive_words}")

            return True

        except pymssql.IntegrityError:
            # å¦‚æœé‡åˆ°é‡å¤é”®é”™è¯¯ï¼Œè·³è¿‡
            logger.info(f"â­ï¸ è·³è¿‡é‡å¤è®°å½• - è®¢å•: {record['order_id']}")
            conn.rollback()
            return False
        except Exception as e:
            logger.warning(f"æ’å…¥æ•°æ®æ—¶é‡åˆ°é”™è¯¯ï¼Œè·³è¿‡è¯¥æ¡è®°å½•: {e}")
            conn.rollback()
            return False

    except Exception as e:
        logger.error(f"æ’å…¥æ•°æ®å¤±è´¥: {e}")
        conn.rollback()
        return False

def process_single_order_record(conn, order_record):
    """å¤„ç†å•æ¡è®¢å•è®°å½•ï¼ˆåŒ…å«é‡‘é¢å­—æ®µï¼‰"""
    try:
        logger.info(f"\næ­£åœ¨å¤„ç†è®¢å•è®°å½•...")
        logger.info(f"  è®¢å•ID: {order_record['order_id']}")
        logger.info(f"  ç”¨æˆ·ID: {order_record['user_id']}")
        logger.info(f"  äº§å“åç§°: {order_record['product_name']}")
        logger.info(f"  é”€å”®é‡‘é¢: {order_record.get('sale_amount', 0.00)}")
        logger.info(f"  æ€»é‡‘é¢: {order_record.get('total_amount', 0.00)}")
        if 'ds' in order_record:
            logger.info(f"  æ—¥æœŸ: {order_record['ds']}")

        # å¤„ç†äº§å“åç§°æ‹†åˆ†
        product_info = split_product_info(order_record['product_name'])

        # ä½¿ç”¨AIç”Ÿæˆè¯„è®ºï¼Œå¹¶è·å–æ•æ„Ÿè¯æ£€æµ‹ç»“æœ
        review, has_sensitive, detected_words = generate_ai_review(
            product_info,
            order_record['user_id'],
            order_record['order_id'],
            conn
        )

        # æ„å»ºç»“æœè®°å½•
        result_record = {
            'order_id': order_record['order_id'],
            'user_id': order_record['user_id'],
            'product_id': order_record['product_id'],
            'product_name': order_record['product_name'],
            'brand': product_info['brand'],
            'english_name': product_info['english_name'],
            'chinese_name': product_info['chinese_name'],
            'ai_review': review,
            'has_sensitive_content': has_sensitive,
            'sensitive_words': ','.join(detected_words) if detected_words else '',
            'sale_amount': order_record.get('sale_amount', 0.00),  # æ·»åŠ é”€å”®é‡‘é¢
            'total_amount': order_record.get('total_amount', 0.00),  # æ·»åŠ æ€»é‡‘é¢
            'ds': order_record.get('ds', None)
        }

        # æ’å…¥åˆ°æ•°æ®åº“
        success = insert_single_record_to_sqlserver(conn, result_record)

        if success:
            logger.info(f"âœ… è®¢å• {order_record['order_id']} å¤„ç†å®Œæˆ")
            if has_sensitive:
                logger.warning(f"ğŸš¨ è¯¥è®¢å•åŒ…å«æ•æ„Ÿè¯: {detected_words}")
        else:
            logger.error(f"âŒ è®¢å• {order_record['order_id']} å¤„ç†å¤±è´¥")

        return success

    except Exception as e:
        logger.error(f"å¤„ç†è®¢å•è®°å½•å¤±è´¥: {e}")
        return False

def get_table_structure(conn, table_name):
    """è·å–è¡¨ç»“æ„ä¿¡æ¯"""
    try:
        cursor = conn.cursor()
        query_sql = f"""
        SELECT COLUMN_NAME, DATA_TYPE 
        FROM INFORMATION_SCHEMA.COLUMNS 
        WHERE TABLE_NAME = '{table_name}'
        ORDER BY ORDINAL_POSITION
        """
        cursor.execute(query_sql)
        columns = cursor.fetchall()

        logger.info(f"è¡¨ {table_name} ç»“æ„:")
        for column in columns:
            logger.info(f"  - {column[0]}: {column[1]}")

        return [col[0] for col in columns]
    except Exception as e:
        logger.error(f"è·å–è¡¨ç»“æ„å¤±è´¥: {e}")
        return []

def get_new_orders(conn, last_processed_time=None, batch_size=10):
    """è·å–æ–°çš„è®¢å•è®°å½•ï¼ˆåŸºäºæ—¶é—´æˆ³çš„å®æ—¶å¤„ç†æ¨¡å¼ï¼‰- åŒ…å«é‡‘é¢å­—æ®µ"""
    try:
        cursor = conn.cursor()

        # å¦‚æœæ²¡æœ‰æœ€åå¤„ç†æ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´å‡å»1å°æ—¶
        if last_processed_time is None:
            last_processed_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        # é¦–å…ˆæ£€æŸ¥oms_order_dtlè¡¨çš„ç»“æ„
        columns = get_table_structure(conn, 'oms_order_dtl')

        # æ„å»ºæŸ¥è¯¢ï¼ŒåŸºäºå¯ç”¨çš„æ—¶é—´å­—æ®µ
        time_field = None
        if 'created_time' in columns:
            time_field = 'created_time'
        elif 'update_time' in columns:
            time_field = 'update_time'
        elif 'ds' in columns:
            time_field = 'ds'

        # æ£€æŸ¥æºè¡¨æ˜¯å¦æœ‰é‡‘é¢å­—æ®µ
        has_sale_amount = 'sale_amount' in columns
        has_total_amount = 'total_amount' in columns

        # é€‰æ‹©é‡‘é¢å­—æ®µï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨NULL
        sale_amount_field = 'sale_amount' if has_sale_amount else 'NULL as sale_amount'
        total_amount_field = 'total_amount' if has_total_amount else 'NULL as total_amount'

        if time_field:
            # ä½¿ç”¨æ—¶é—´å­—æ®µè¿›è¡ŒæŸ¥è¯¢
            query_sql = f"""
            SELECT TOP {batch_size} 
                order_id,
                user_id,
                product_id,
                product_name,
                {sale_amount_field},
                {total_amount_field},
                ds
            FROM oms_order_dtl 
            WHERE {time_field} > '{last_processed_time}'
            ORDER BY {time_field} ASC
            """
        else:
            # å¦‚æœæ²¡æœ‰æ—¶é—´å­—æ®µï¼Œä½¿ç”¨å›ºå®šæŸ¥è¯¢
            logger.warning("æœªæ‰¾åˆ°æ—¶é—´å­—æ®µï¼Œä½¿ç”¨å›ºå®šæŸ¥è¯¢")
            query_sql = f"""
            SELECT TOP {batch_size} 
                order_id,
                user_id,
                product_id,
                product_name,
                {sale_amount_field},
                {total_amount_field},
                ds
            FROM oms_order_dtl 
            ORDER BY order_id
            """

        cursor.execute(query_sql)
        orders = cursor.fetchall()

        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        order_list = []
        for order in orders:
            order_dict = {
                'order_id': order[0],
                'user_id': order[1],
                'product_id': order[2],
                'product_name': order[3],
                'sale_amount': float(order[4]) if order[4] is not None else 0.00,
                'total_amount': float(order[5]) if order[5] is not None else 0.00,
                'ds': order[6] if len(order) > 6 else None
            }
            order_list.append(order_dict)

        logger.info(f"è·å–åˆ° {len(order_list)} æ¡æ–°è®¢å•")
        # å¦‚æœæœ‰é‡‘é¢æ•°æ®ï¼Œè®°å½•æ—¥å¿—
        if has_sale_amount or has_total_amount:
            logger.info(f"é‡‘é¢å­—æ®µçŠ¶æ€: sale_amount={has_sale_amount}, total_amount={has_total_amount}")
        return order_list

    except Exception as e:
        logger.error(f"è·å–æ–°è®¢å•å¤±è´¥: {e}")
        return []

def get_last_processed_time(conn):
    """è·å–æœ€åå¤„ç†çš„æ—¶é—´"""
    try:
        cursor = conn.cursor()

        # ä»ç›®æ ‡è¡¨è·å–æœ€åå¤„ç†çš„æ—¶é—´
        query_sql = """
        SELECT MAX(created_time) 
        FROM oms_order_dtl_enhanced2
        """

        cursor.execute(query_sql)
        result = cursor.fetchone()

        if result and result[0]:
            return result[0].strftime('%Y-%m-%d %H:%M:%S')
        else:
            # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œè¿”å›1å°æ—¶å‰çš„æ—¶é—´
            one_hour_ago = datetime.now().replace(hour=datetime.now().hour-1)
            return one_hour_ago.strftime('%Y-%m-%d %H:%M:%S')

    except Exception as e:
        logger.error(f"è·å–æœ€åå¤„ç†æ—¶é—´å¤±è´¥: {e}")
        # è¿”å›1å°æ—¶å‰çš„æ—¶é—´ä½œä¸ºé»˜è®¤å€¼
        one_hour_ago = datetime.now().replace(hour=datetime.now().hour-1)
        return one_hour_ago.strftime('%Y-%m-%d %H:%M:%S')

def real_time_processing_loop(conn, check_interval=30, batch_size=5):
    """å®æ—¶å¤„ç†å¾ªç¯"""
    logger.info("ğŸš€ å¯åŠ¨å®æ—¶å¤„ç†æ¨¡å¼...")

    # è·å–æœ€åå¤„ç†çš„æ—¶é—´
    last_processed_time = get_last_processed_time(conn)
    logger.info(f"ğŸ“Š æœ€åå¤„ç†æ—¶é—´: {last_processed_time}")

    processed_count = 0
    error_count = 0

    while True:
        try:
            # è·å–æ–°è®¢å•
            new_orders = get_new_orders(conn, last_processed_time, batch_size)

            if new_orders:
                logger.info(f"ğŸ“¥ è·å–åˆ° {len(new_orders)} æ¡æ–°è®¢å•")

                for order in new_orders:
                    # å¤„ç†å•æ¡è®¢å•
                    success = process_single_order_record(conn, order)

                    if success:
                        processed_count += 1
                    else:
                        error_count += 1
                        # æ— è®ºæˆåŠŸå¤±è´¥éƒ½æ›´æ–°æ—¶é—´æˆ³ï¼Œé¿å…é‡å¤å¤„ç†
                    last_processed_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    logger.info("ç­‰å¾…3ç§’...")
                    time.sleep(3)

                logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡ - æˆåŠŸ: {processed_count}, å¤±è´¥: {error_count}")

                # å¤„ç†è¿è§„ç”¨æˆ·
                logger.info("å¼€å§‹å¤„ç†è¿è§„ç”¨æˆ·...")
                process_violation_users(conn)

            else:
                logger.info(f"â³ æ²¡æœ‰æ–°è®¢å•ï¼Œç­‰å¾… {check_interval} ç§’åé‡è¯•...")

            # ç­‰å¾…æŒ‡å®šé—´éš”
            time.sleep(check_interval)

        except KeyboardInterrupt:
            logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­å¤„ç†")
            break
        except Exception as e:
            logger.error(f"å®æ—¶å¤„ç†å¾ªç¯å¼‚å¸¸: {e}")
            error_count += 1
            time.sleep(check_interval)  # å‡ºé”™æ—¶ä¹Ÿç­‰å¾…

def main():
    """ä¸»å‡½æ•° - å®æ—¶å¤„ç†æ¨¡å¼"""
    try:
        # è¿æ¥SQL Serveræ•°æ®åº“
        conn = pymssql.connect(
            server=mysql_ip,
            user=mysql_user_name,
            password=mysql_user_pwd,
            database=mysql_order_db,
            port=int(mysql_port) if mysql_port else 1433,
            charset='UTF-8'
        )
        logger.info("SQL Serveræ•°æ®åº“è¿æ¥æˆåŠŸ!")

        # æ˜¾ç¤ºåˆ†ç±»å™¨ä¿¡æ¯
        stats = enhanced_classifier.get_statistics()
        logger.info(f"ğŸ¯ å¢å¼ºç‰ˆæ•æ„Ÿè¯åˆ†ç±»å™¨ç»Ÿè®¡:")
        logger.info(f"  - P0ä¸¥é‡è¿è§„: {stats['p0_count']}ä¸ª")
        logger.info(f"  - P1è„è¯æ­§è§†: {stats['p1_count']}ä¸ª")
        logger.info(f"  - P2ä¸€èˆ¬æ•æ„Ÿ: {stats['p2_count']}ä¸ª")
        logger.info(f"  - æ€»è®¡: {stats['total_count']}ä¸ª")
        logger.info(f"  - æ£€æµ‹æ–¹æ³•: {'IKåˆ†è¯å™¨' if enhanced_classifier.use_ik_analyzer else 'æ™®é€šåŒ¹é…'}")
        logger.info(f"  - IKåˆ†è¯å™¨çŠ¶æ€: {'å·²å¯ç”¨' if ik_analyzer.initialized else 'æœªå¯ç”¨'}")
        logger.info(f"  - jiebaçŠ¶æ€: {'âœ… å¯ç”¨' if JIEBA_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")

        # æ£€æŸ¥å¹¶åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰- ä¿®å¤ç‰ˆæœ¬
        check_and_create_table(conn)

        # å¯åŠ¨å®æ—¶å¤„ç†å¾ªç¯
        real_time_processing_loop(conn, check_interval=30, batch_size=5)

        conn.close()
        logger.info("\nğŸ‰ å®æ—¶å¤„ç†ç»“æŸï¼")

    except Exception as e:
        logger.error(f"å®æ—¶å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()